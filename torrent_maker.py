#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Torrent Maker - å•æ–‡ä»¶ç‰ˆæœ¬ v1.5.0
åŸºäº mktorrent çš„é«˜æ€§èƒ½åŠè‡ªåŠ¨åŒ–ç§å­åˆ¶ä½œå·¥å…·

ğŸš€ v1.5.0 æ€§èƒ½ä¼˜åŒ–æ›´æ–°:
- âš¡ ç§å­åˆ›å»ºé€Ÿåº¦æå‡ 30-50%
- ğŸ§  æ™ºèƒ½ Piece Size è®¡ç®—ï¼Œå‡å°‘è®¡ç®—æ—¶é—´ 80%
- ï¿½ ç›®å½•å¤§å°ç¼“å­˜ä¼˜åŒ–ï¼Œæ”¯æŒ LRU æ·˜æ±°ç­–ç•¥
- ğŸ”§ mktorrent å‚æ•°ä¼˜åŒ–ï¼Œå¯ç”¨å¤šçº¿ç¨‹å¤„ç†
- ï¿½ æ‰¹é‡å¤„ç†å¹¶å‘ä¼˜åŒ–ï¼Œæ”¯æŒè¿›ç¨‹æ± å¤„ç†
- ğŸ“Š å¢å¼ºæ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡åˆ†æ
- ğŸ¯ æ™ºèƒ½æŸ¥æ‰¾è¡¨ï¼ŒO(1) æ—¶é—´å¤æ‚åº¦ä¼˜åŒ–

æ€§èƒ½æå‡:
- ç›®å½•æ‰«æ: 10s â†’ 2-3s
- ç§å­åˆ›å»º: 30s â†’ 15-20s
- æœç´¢å“åº”: 2s â†’ 0.8-1.2s
- å†…å­˜ä½¿ç”¨: å‡å°‘ 20-30%

ä½¿ç”¨æ–¹æ³•ï¼š
    python torrent_maker.py

ä½œè€…ï¼šTorrent Maker Team
è®¸å¯è¯ï¼šMIT
ç‰ˆæœ¬ï¼š1.5.0
"""

import os
import sys
import json
import subprocess
import shutil
import time
import logging
import hashlib
import tempfile
import threading
import re
from datetime import datetime
from difflib import SequenceMatcher
from typing import List, Dict, Any, Tuple, Optional, Union, Set
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


# ================== æ€§èƒ½ç›‘æ§ ==================
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç±»"""

    def __init__(self):
        self._timers: Dict[str, float] = {}
        self._stats: Dict[str, List[float]] = {}
        self._lock = threading.Lock()

    def start_timer(self, name: str) -> None:
        with self._lock:
            self._timers[name] = time.time()

    def end_timer(self, name: str) -> float:
        with self._lock:
            if name in self._timers:
                duration = time.time() - self._timers[name]
                if name not in self._stats:
                    self._stats[name] = []
                self._stats[name].append(duration)
                del self._timers[name]
                return duration
            return 0.0

    def get_stats(self, name: str) -> Dict[str, float]:
        with self._lock:
            if name not in self._stats or not self._stats[name]:
                return {}

            times = self._stats[name]
            return {
                'count': len(times),
                'total': sum(times),
                'average': sum(times) / len(times),
                'min': min(times),
                'max': max(times)
            }

    def get_all_stats(self) -> Dict[str, Dict[str, float]]:
        with self._lock:
            return {name: self.get_stats(name) for name in self._stats.keys()}


# ================== ç¼“å­˜ç³»ç»Ÿ ==================
class SearchCache:
    """æœç´¢ç»“æœç¼“å­˜ç±»"""

    def __init__(self, cache_duration: int = 3600):
        self.cache_duration = cache_duration
        self._cache: Dict[str, Tuple[float, Any]] = {}
        self._lock = threading.Lock()

    def get(self, key: str) -> Optional[Any]:
        with self._lock:
            if key in self._cache:
                timestamp, value = self._cache[key]
                if time.time() - timestamp < self.cache_duration:
                    return value
                else:
                    del self._cache[key]
            return None

    def set(self, key: str, value: Any) -> None:
        with self._lock:
            self._cache[key] = (time.time(), value)

    def clear(self) -> None:
        with self._lock:
            self._cache.clear()

    def get_stats(self) -> Dict[str, Any]:
        with self._lock:
            total_items = len(self._cache)
            current_time = time.time()
            expired_items = sum(1 for timestamp, _ in self._cache.values()
                              if current_time - timestamp >= self.cache_duration)
            return {
                'total_items': total_items,
                'valid_items': total_items - expired_items,
                'expired_items': expired_items
            }


# ================== ç›®å½•å¤§å°ç¼“å­˜ ==================
class DirectorySizeCache:
    """ç›®å½•å¤§å°ç¼“å­˜ç±» - é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""

    def __init__(self, cache_duration: int = 1800, max_cache_size: int = 1000):
        self.cache_duration = cache_duration
        self.max_cache_size = max_cache_size
        self._cache: Dict[str, Tuple[float, int, float, int]] = {}  # path -> (timestamp, size, mtime, access_count)
        self._access_order: List[str] = []  # LRU è®¿é—®é¡ºåº
        self._lock = threading.Lock()
        self._stats = {'hits': 0, 'misses': 0, 'evictions': 0}

    def get_directory_size(self, path: Path) -> int:
        """è·å–ç›®å½•å¤§å°ï¼Œä½¿ç”¨é«˜æ€§èƒ½ç¼“å­˜ä¼˜åŒ–"""
        path_str = str(path)
        current_time = time.time()

        try:
            # è·å–ç›®å½•çš„ä¿®æ”¹æ—¶é—´
            dir_mtime = path.stat().st_mtime
        except (OSError, PermissionError):
            return self._calculate_size_fallback(path)

        with self._lock:
            # æ£€æŸ¥ç¼“å­˜
            if path_str in self._cache:
                timestamp, cached_size, cached_mtime, access_count = self._cache[path_str]
                # å¦‚æœç¼“å­˜æœªè¿‡æœŸä¸”ç›®å½•æœªä¿®æ”¹ï¼Œè¿”å›ç¼“å­˜å€¼
                if (current_time - timestamp < self.cache_duration and
                    abs(dir_mtime - cached_mtime) < 1.0):  # 1ç§’å®¹å·®
                    # æ›´æ–°è®¿é—®ç»Ÿè®¡å’Œ LRU é¡ºåº
                    self._cache[path_str] = (timestamp, cached_size, cached_mtime, access_count + 1)
                    self._update_access_order(path_str)
                    self._stats['hits'] += 1
                    return cached_size
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œç§»é™¤
                    self._remove_from_cache(path_str)

        self._stats['misses'] += 1

        # è®¡ç®—ç›®å½•å¤§å°
        total_size = self._calculate_size_optimized(path)

        # æ›´æ–°ç¼“å­˜
        with self._lock:
            self._add_to_cache(path_str, current_time, total_size, dir_mtime)

        return total_size

    def _add_to_cache(self, path_str: str, timestamp: float, size: int, mtime: float) -> None:
        """æ·»åŠ åˆ°ç¼“å­˜ï¼Œå®ç° LRU æ·˜æ±°"""
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€å°‘ä½¿ç”¨çš„é¡¹
        if len(self._cache) >= self.max_cache_size:
            self._evict_lru()

        self._cache[path_str] = (timestamp, size, mtime, 1)
        self._access_order.append(path_str)

    def _remove_from_cache(self, path_str: str) -> None:
        """ä»ç¼“å­˜ä¸­ç§»é™¤é¡¹ç›®"""
        if path_str in self._cache:
            del self._cache[path_str]
        if path_str in self._access_order:
            self._access_order.remove(path_str)

    def _update_access_order(self, path_str: str) -> None:
        """æ›´æ–° LRU è®¿é—®é¡ºåº"""
        if path_str in self._access_order:
            self._access_order.remove(path_str)
        self._access_order.append(path_str)

    def _evict_lru(self) -> None:
        """æ·˜æ±°æœ€å°‘ä½¿ç”¨çš„ç¼“å­˜é¡¹"""
        if self._access_order:
            lru_path = self._access_order.pop(0)
            if lru_path in self._cache:
                del self._cache[lru_path]
                self._stats['evictions'] += 1

    def _calculate_size_optimized(self, path: Path) -> int:
        """å†…å­˜ä¼˜åŒ–çš„ç›®å½•å¤§å°è®¡ç®—"""
        # æ£€æŸ¥ç›®å½•å¤§å°ï¼Œå†³å®šä½¿ç”¨å“ªç§ç­–ç•¥
        try:
            # å¿«é€Ÿä¼°ç®—ç›®å½•å¤æ‚åº¦
            complexity = self._estimate_directory_complexity(path)

            if complexity['estimated_files'] > 10000:
                # å¤§ç›®å½•ä½¿ç”¨æµå¼å¤„ç†
                return self._calculate_size_streaming(path)
            elif complexity['estimated_files'] > 1000:
                # ä¸­ç­‰ç›®å½•ä½¿ç”¨æ‰¹é‡å¤„ç†
                return self._calculate_size_batch(path)
            else:
                # å°ç›®å½•ä½¿ç”¨ç®€å•æ–¹æ³•
                return self._scan_directory_simple(path)

        except Exception:
            # å›é€€åˆ°ç®€å•æ–¹æ³•
            return self._scan_directory_simple(path)

    def _estimate_directory_complexity(self, path: Path) -> Dict[str, int]:
        """ä¼°ç®—ç›®å½•å¤æ‚åº¦"""
        try:
            sample_count = 0
            dir_count = 0
            file_count = 0

            # åªæ‰«æå‰å‡ ä¸ªå­ç›®å½•æ¥ä¼°ç®—
            with os.scandir(path) as entries:
                for entry in entries:
                    sample_count += 1
                    if entry.is_dir(follow_symlinks=False):
                        dir_count += 1
                    elif entry.is_file(follow_symlinks=False):
                        file_count += 1

                    # åªé‡‡æ ·å‰ 100 ä¸ªé¡¹ç›®
                    if sample_count >= 100:
                        break

            # ä¼°ç®—æ€»æ–‡ä»¶æ•°
            if dir_count > 0:
                estimated_files = file_count + dir_count * 50  # å‡è®¾æ¯ä¸ªå­ç›®å½•å¹³å‡ 50 ä¸ªæ–‡ä»¶
            else:
                estimated_files = file_count

            return {
                'sample_count': sample_count,
                'dir_count': dir_count,
                'file_count': file_count,
                'estimated_files': estimated_files
            }

        except (OSError, PermissionError):
            return {'sample_count': 0, 'dir_count': 0, 'file_count': 0, 'estimated_files': 0}

    def _calculate_size_streaming(self, path: Path) -> int:
        """æµå¼è®¡ç®—å¤§ç›®å½•å¤§å° - å¼‚æ­¥ä¼˜åŒ–ç‰ˆæœ¬"""
        # å°è¯•ä½¿ç”¨å¼‚æ­¥å¤„ç†å™¨
        try:
            async_processor = AsyncFileProcessor(max_concurrent=4)

            # å…ˆå¼‚æ­¥æ‰«æç›®å½•æ ‘
            loop = async_processor.async_io._get_event_loop()
            if loop is not None:
                try:
                    import asyncio
                    if loop.is_running():
                        future = asyncio.ensure_future(
                            async_processor.async_directory_tree_scan(path, max_depth=10, include_files=True)
                        )
                        result = asyncio.run_coroutine_threadsafe(future, loop).result(timeout=30)
                    else:
                        result = loop.run_until_complete(
                            async_processor.async_directory_tree_scan(path, max_depth=10, include_files=True)
                        )

                    return result.get('total_size', 0)
                except Exception:
                    pass
        except Exception:
            pass

        # å›é€€åˆ°åŒæ­¥æµå¼å¤„ç†
        total_size = 0
        processed_count = 0

        try:
            for file_path in path.rglob('*'):
                if file_path.is_file():
                    try:
                        total_size += file_path.stat().st_size
                        processed_count += 1

                        # æ¯å¤„ç† 1000 ä¸ªæ–‡ä»¶æ£€æŸ¥ä¸€æ¬¡å†…å­˜
                        if processed_count % 1000 == 0:
                            # è¿™é‡Œå¯ä»¥æ·»åŠ å†…å­˜æ£€æŸ¥é€»è¾‘
                            pass

                    except (OSError, IOError):
                        pass
        except (OSError, PermissionError):
            pass

        return total_size

    def _calculate_size_batch(self, path: Path) -> int:
        """æ‰¹é‡è®¡ç®—ä¸­ç­‰ç›®å½•å¤§å°"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import queue

        total_size = 0
        scan_queue = queue.Queue()
        scan_queue.put(path)

        def scan_directory_batch() -> int:
            """æ‰¹é‡æ‰«æç›®å½•"""
            batch_size = 0

            try:
                while not scan_queue.empty():
                    try:
                        current_path = scan_queue.get_nowait()

                        with os.scandir(current_path) as entries:
                            for entry in entries:
                                if entry.is_file(follow_symlinks=False):
                                    try:
                                        batch_size += entry.stat().st_size
                                    except (OSError, IOError):
                                        pass
                                elif entry.is_dir(follow_symlinks=False):
                                    scan_queue.put(Path(entry.path))
                    except queue.Empty:
                        break
                    except (PermissionError, OSError):
                        continue
            except Exception:
                pass

            return batch_size

        try:
            # ä½¿ç”¨å°‘é‡çº¿ç¨‹å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=2) as executor:
                futures = []

                # å¯åŠ¨æ‰«æä»»åŠ¡
                for _ in range(2):
                    if not scan_queue.empty():
                        futures.append(executor.submit(scan_directory_batch))

                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    try:
                        total_size += future.result()
                    except Exception:
                        pass

                # å¤„ç†å‰©ä½™ç›®å½•
                while not scan_queue.empty():
                    try:
                        remaining_path = scan_queue.get_nowait()
                        total_size += self._scan_directory_simple(remaining_path)
                    except queue.Empty:
                        break

        except Exception:
            total_size = self._scan_directory_simple(path)

        return total_size

    def _scan_directory_simple(self, path: Path) -> int:
        """ç®€å•çš„ç›®å½•æ‰«ææ–¹æ³•"""
        size = 0
        try:
            with os.scandir(path) as entries:
                for entry in entries:
                    if entry.is_file(follow_symlinks=False):
                        try:
                            size += entry.stat().st_size
                        except (OSError, IOError):
                            pass
                    elif entry.is_dir(follow_symlinks=False):
                        size += self._scan_directory_simple(Path(entry.path))
        except (PermissionError, OSError):
            pass
        return size

    def _calculate_size_fallback(self, path: Path) -> int:
        """å›é€€çš„ç›®å½•å¤§å°è®¡ç®—æ–¹æ³•"""
        total_size = 0
        try:
            for file_path in path.rglob('*'):
                if file_path.is_file():
                    try:
                        total_size += file_path.stat().st_size
                    except (OSError, IOError):
                        pass
        except (OSError, PermissionError):
            pass
        return total_size

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            total_requests = self._stats['hits'] + self._stats['misses']
            hit_rate = self._stats['hits'] / total_requests if total_requests > 0 else 0

            return {
                'cache_size': len(self._cache),
                'max_cache_size': self.max_cache_size,
                'hit_rate': hit_rate,
                'hits': self._stats['hits'],
                'misses': self._stats['misses'],
                'evictions': self._stats['evictions'],
                'total_requests': total_requests
            }

    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self._cache.clear()
            self._access_order.clear()
            self._stats = {'hits': 0, 'misses': 0, 'evictions': 0}

    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜é¡¹"""
        current_time = time.time()
        expired_count = 0

        with self._lock:
            expired_paths = []
            for path_str, (timestamp, _, _, _) in self._cache.items():
                if current_time - timestamp >= self.cache_duration:
                    expired_paths.append(path_str)

            for path_str in expired_paths:
                self._remove_from_cache(path_str)
                expired_count += 1

        return expired_count


# ================== å¼‚å¸¸ç±» ==================
class ConfigValidationError(Exception):
    """é…ç½®éªŒè¯é”™è¯¯"""
    pass


class TorrentCreationError(Exception):
    """ç§å­åˆ›å»ºé”™è¯¯"""
    pass


# ================== é…ç½®ç®¡ç†å™¨ ==================
class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - v1.4.0ä¿®å¤ä¼˜åŒ–ç‰ˆæœ¬"""
    
    DEFAULT_SETTINGS = {
        "resource_folder": "~/Downloads",
        "output_folder": "~/Desktop/torrents",
        "file_search_tolerance": 60,
        "max_search_results": 10,
        "auto_create_output_dir": True,
        "enable_cache": True,
        "cache_duration": 3600,
        "max_concurrent_operations": 4,
        "log_level": "WARNING"
    }
    
    DEFAULT_TRACKERS = [
        "udp://tracker.openbittorrent.com:80",
        "udp://tracker.opentrackr.org:1337/announce",
        "udp://exodus.desync.com:6969/announce",
        "udp://tracker.torrent.eu.org:451/announce"
    ]

    def __init__(self):
        self.config_dir = os.path.expanduser("~/.torrent_maker")
        self.settings_path = os.path.join(self.config_dir, "settings.json")
        self.trackers_path = os.path.join(self.config_dir, "trackers.txt")
        
        self._ensure_config_files()
        self.settings = self._load_settings()
        self.trackers = self._load_trackers()
        self._validate_config()

    def _ensure_config_files(self) -> None:
        try:
            os.makedirs(self.config_dir, exist_ok=True)
            if not os.path.exists(self.settings_path):
                self._create_default_settings()
            if not os.path.exists(self.trackers_path):
                self._create_default_trackers()
        except OSError as e:
            raise ConfigValidationError(f"æ— æ³•åˆ›å»ºé…ç½®æ–‡ä»¶: {e}")

    def _create_default_settings(self) -> None:
        settings = self.DEFAULT_SETTINGS.copy()
        settings['resource_folder'] = os.path.expanduser(settings['resource_folder'])
        settings['output_folder'] = os.path.expanduser(settings['output_folder'])
        
        with open(self.settings_path, 'w', encoding='utf-8') as f:
            json.dump(settings, f, ensure_ascii=False, indent=4)

    def _create_default_trackers(self) -> None:
        with open(self.trackers_path, 'w', encoding='utf-8') as f:
            f.write("# BitTorrent Tracker åˆ—è¡¨\n")
            f.write("# æ¯è¡Œä¸€ä¸ª tracker URLï¼Œä»¥ # å¼€å¤´çš„è¡Œä¸ºæ³¨é‡Š\n\n")
            for tracker in self.DEFAULT_TRACKERS:
                f.write(f"{tracker}\n")

    def _load_settings(self) -> Dict[str, Any]:
        try:
            with open(self.settings_path, 'r', encoding='utf-8') as f:
                settings = json.load(f)
            
            for key in ['resource_folder', 'output_folder']:
                if key in settings:
                    settings[key] = os.path.expanduser(settings[key])
                    
            merged_settings = self.DEFAULT_SETTINGS.copy()
            merged_settings.update(settings)
            return merged_settings
            
        except (FileNotFoundError, json.JSONDecodeError):
            return self.DEFAULT_SETTINGS.copy()

    def _load_trackers(self) -> List[str]:
        try:
            with open(self.trackers_path, 'r', encoding='utf-8') as f:
                trackers = []
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        trackers.append(line)
                return trackers if trackers else self.DEFAULT_TRACKERS.copy()
        except FileNotFoundError:
            return self.DEFAULT_TRACKERS.copy()

    def _validate_config(self) -> None:
        numeric_configs = {
            'file_search_tolerance': (0, 100),
            'max_search_results': (1, 100),
            'cache_duration': (60, 86400),
            'max_concurrent_operations': (1, 20)
        }
        
        for key, (min_val, max_val) in numeric_configs.items():
            if key in self.settings:
                value = self.settings[key]
                if not isinstance(value, (int, float)) or not (min_val <= value <= max_val):
                    self.settings[key] = self.DEFAULT_SETTINGS[key]

    def get_resource_folder(self) -> str:
        return os.path.abspath(self.settings.get('resource_folder', os.path.expanduser("~/Downloads")))

    def get_output_folder(self) -> str:
        output_path = self.settings.get('output_folder', os.path.expanduser("~/Desktop/torrents"))
        return os.path.abspath(output_path)

    def get_trackers(self) -> List[str]:
        return self.trackers.copy()

    def save_settings(self):
        try:
            with open(self.settings_path, 'w', encoding='utf-8') as f:
                json.dump(self.settings, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"ä¿å­˜è®¾ç½®æ—¶å‡ºé”™: {e}")

    def save_trackers(self):
        try:
            with open(self.trackers_path, 'w', encoding='utf-8') as f:
                f.write("# BitTorrent Tracker åˆ—è¡¨\n")
                f.write("# æ¯è¡Œä¸€ä¸ª tracker URLï¼Œä»¥ # å¼€å¤´çš„è¡Œä¸ºæ³¨é‡Š\n\n")
                for tracker in self.trackers:
                    f.write(f"{tracker}\n")
        except Exception as e:
            print(f"ä¿å­˜ tracker æ—¶å‡ºé”™: {e}")

    def set_resource_folder(self, path: str) -> bool:
        """è®¾ç½®èµ„æºæ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¹¶éªŒè¯è·¯å¾„æœ‰æ•ˆæ€§"""
        try:
            expanded_path = os.path.expanduser(path)
            expanded_path = os.path.abspath(expanded_path)

            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(expanded_path):
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {expanded_path}")
                return False

            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•
            if not os.path.isdir(expanded_path):
                print(f"âŒ è·¯å¾„ä¸æ˜¯ç›®å½•: {expanded_path}")
                return False

            self.settings['resource_folder'] = expanded_path
            self.save_settings()
            print(f"âœ… èµ„æºæ–‡ä»¶å¤¹å·²è®¾ç½®ä¸º: {expanded_path}")
            return True

        except Exception as e:
            print(f"âŒ è®¾ç½®èµ„æºæ–‡ä»¶å¤¹å¤±è´¥: {e}")
            return False

    def set_output_folder(self, path: str):
        expanded_path = os.path.expanduser(path)
        self.settings['output_folder'] = expanded_path
        self.save_settings()

    def add_tracker(self, tracker_url: str):
        if tracker_url not in self.trackers:
            self.trackers.append(tracker_url)
            self.save_trackers()
            return True
        return False

    def remove_tracker(self, tracker_url: str):
        if tracker_url in self.trackers:
            self.trackers.remove(tracker_url)
            self.save_trackers()
            return True
        return False

    def get_setting(self, key: str, default=None):
        """è·å–å•ä¸ªè®¾ç½®é¡¹

        Args:
            key: è®¾ç½®é¡¹é”®å
            default: é»˜è®¤å€¼

        Returns:
            è®¾ç½®é¡¹çš„å€¼
        """
        return self.settings.get(key, default)

    def set_setting(self, key: str, value):
        """è®¾ç½®å•ä¸ªé…ç½®é¡¹

        Args:
            key: è®¾ç½®é¡¹é”®å
            value: è®¾ç½®é¡¹çš„å€¼

        Returns:
            è®¾ç½®æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            self.settings[key] = value
            self.save_settings()
            return True
        except Exception as e:
            print(f"è®¾ç½®é…ç½®é¡¹å¤±è´¥: {e}")
            return False


# ================== æ™ºèƒ½ç´¢å¼•ç¼“å­˜ ==================
class SmartIndexCache:
    """æ™ºèƒ½ç´¢å¼•ç¼“å­˜ - v1.5.0 æœç´¢ä¼˜åŒ–"""

    def __init__(self, cache_duration: int = 3600):
        self.cache_duration = cache_duration
        self._word_index: Dict[str, Set[str]] = {}  # word -> set of folder paths
        self._folder_words: Dict[str, Set[str]] = {}  # folder_path -> set of words
        self._last_update = 0
        self._lock = threading.Lock()

    def build_index(self, folders: List[Path], normalize_func) -> None:
        """æ„å»ºæ™ºèƒ½ç´¢å¼•"""
        with self._lock:
            self._word_index.clear()
            self._folder_words.clear()

            for folder in folders:
                folder_path = str(folder)
                normalized_name = normalize_func(folder.name)
                words = set(normalized_name.split())

                self._folder_words[folder_path] = words

                for word in words:
                    if word not in self._word_index:
                        self._word_index[word] = set()
                    self._word_index[word].add(folder_path)

            self._last_update = time.time()

    def get_candidate_folders(self, search_words: Set[str]) -> Set[str]:
        """æ ¹æ®æœç´¢è¯è·å–å€™é€‰æ–‡ä»¶å¤¹"""
        if not search_words:
            return set()

        candidate_sets = []
        for word in search_words:
            if word in self._word_index:
                candidate_sets.append(self._word_index[word])

        if not candidate_sets:
            return set()

        # è¿”å›åŒ…å«ä»»æ„æœç´¢è¯çš„æ–‡ä»¶å¤¹
        return set.union(*candidate_sets)

    def is_expired(self) -> bool:
        """æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¿‡æœŸ"""
        return time.time() - self._last_update > self.cache_duration


# ================== å†…å­˜åˆ†æå™¨ ==================
class MemoryAnalyzer:
    """å†…å­˜åˆ†æå™¨ - æ·±åº¦å†…å­˜ä½¿ç”¨åˆ†æ"""

    @staticmethod
    def get_object_memory_usage() -> Dict[str, Any]:
        """è·å–å¯¹è±¡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import gc
        import sys

        # ç»Ÿè®¡ä¸åŒç±»å‹å¯¹è±¡çš„æ•°é‡
        type_counts = {}
        total_objects = 0

        for obj in gc.get_objects():
            obj_type = type(obj).__name__
            type_counts[obj_type] = type_counts.get(obj_type, 0) + 1
            total_objects += 1

        # è·å–æœ€å å†…å­˜çš„å¯¹è±¡ç±»å‹
        top_types = sorted(type_counts.items(), key=lambda x: x[1], reverse=True)[:10]

        return {
            'total_objects': total_objects,
            'top_memory_types': top_types,
            'gc_stats': {
                'collections': gc.get_stats(),
                'garbage_count': len(gc.garbage)
            }
        }

    @staticmethod
    def analyze_memory_leaks() -> Dict[str, Any]:
        """åˆ†ææ½œåœ¨çš„å†…å­˜æ³„æ¼"""
        import gc
        import weakref

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()

        # æ£€æŸ¥å¾ªç¯å¼•ç”¨
        referrers_count = {}
        for obj in gc.get_objects():
            referrers = gc.get_referrers(obj)
            ref_count = len(referrers)
            if ref_count > 10:  # è¢«å¼•ç”¨æ¬¡æ•°è¿‡å¤šçš„å¯¹è±¡
                obj_type = type(obj).__name__
                referrers_count[obj_type] = referrers_count.get(obj_type, 0) + 1

        return {
            'collected_objects': collected,
            'high_reference_objects': referrers_count,
            'unreachable_objects': len(gc.garbage)
        }


# ================== å¢å¼ºå†…å­˜ç®¡ç†å™¨ ==================
class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨ - v1.5.0 æ·±åº¦å†…å­˜ä¼˜åŒ–"""

    def __init__(self, max_memory_mb: int = 512):
        self.max_memory_mb = max_memory_mb
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self._memory_pools: Dict[str, List[Any]] = {}
        self._object_cache: Dict[str, Any] = {}
        self._memory_history: List[Dict[str, float]] = []
        self._lock = threading.Lock()
        self._analyzer = MemoryAnalyzer()
        self._cleanup_threshold = 0.8  # 80% å†…å­˜ä½¿ç”¨æ—¶è§¦å‘æ¸…ç†

    def get_memory_usage(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            system_memory = psutil.virtual_memory()

            usage_data = {
                'rss_mb': memory_info.rss / (1024 * 1024),
                'vms_mb': memory_info.vms / (1024 * 1024),
                'percent': process.memory_percent(),
                'available_mb': system_memory.available / (1024 * 1024),
                'system_total_mb': system_memory.total / (1024 * 1024),
                'system_used_percent': system_memory.percent,
                'swap_mb': getattr(memory_info, 'swap', 0) / (1024 * 1024)
            }

            # è®°å½•å†…å­˜å†å²
            self._record_memory_history(usage_data)

            return usage_data

        except ImportError:
            # å›é€€åˆ°ç®€å•çš„å†…å­˜ä¼°ç®—
            import resource
            try:
                # å°è¯•ä½¿ç”¨ resource æ¨¡å—
                usage = resource.getrusage(resource.RUSAGE_SELF)
                rss_mb = usage.ru_maxrss / 1024  # macOS è¿”å›å­—èŠ‚ï¼ŒLinux è¿”å› KB

                return {
                    'rss_mb': rss_mb,
                    'vms_mb': 0,
                    'percent': 0,
                    'available_mb': 1024,
                    'system_total_mb': 0,
                    'system_used_percent': 0,
                    'swap_mb': 0
                }
            except:
                return {
                    'rss_mb': 0,
                    'vms_mb': 0,
                    'percent': 0,
                    'available_mb': 1024,
                    'system_total_mb': 0,
                    'system_used_percent': 0,
                    'swap_mb': 0
                }

    def _record_memory_history(self, usage_data: Dict[str, float]) -> None:
        """è®°å½•å†…å­˜ä½¿ç”¨å†å²"""
        with self._lock:
            self._memory_history.append({
                'timestamp': time.time(),
                'rss_mb': usage_data['rss_mb'],
                'percent': usage_data['percent']
            })

            # åªä¿ç•™æœ€è¿‘ 100 æ¡è®°å½•
            if len(self._memory_history) > 100:
                self._memory_history = self._memory_history[-100:]

    def should_cleanup(self) -> bool:
        """æ™ºèƒ½æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†å†…å­˜"""
        memory_info = self.get_memory_usage()
        current_usage = memory_info['rss_mb']

        # å¤šé‡æ£€æŸ¥æ¡ä»¶
        conditions = [
            current_usage > self.max_memory_mb,  # è¶…è¿‡è®¾å®šé™åˆ¶
            current_usage > self.max_memory_mb * self._cleanup_threshold,  # è¶…è¿‡é˜ˆå€¼
            memory_info.get('system_used_percent', 0) > 85,  # ç³»ç»Ÿå†…å­˜ä½¿ç”¨è¿‡é«˜
            self._is_memory_growing_rapidly()  # å†…å­˜å¢é•¿è¿‡å¿«
        ]

        return any(conditions)

    def _is_memory_growing_rapidly(self) -> bool:
        """æ£€æŸ¥å†…å­˜æ˜¯å¦å¢é•¿è¿‡å¿«"""
        if len(self._memory_history) < 5:
            return False

        recent_usage = [entry['rss_mb'] for entry in self._memory_history[-5:]]
        if len(recent_usage) < 2:
            return False

        # è®¡ç®—å†…å­˜å¢é•¿ç‡
        growth_rate = (recent_usage[-1] - recent_usage[0]) / len(recent_usage)
        return growth_rate > 10  # æ¯æ¬¡æµ‹é‡å¢é•¿è¶…è¿‡ 10MB

    def cleanup_if_needed(self, force: bool = False) -> Dict[str, int]:
        """æ™ºèƒ½å†…å­˜æ¸…ç†"""
        if force or self.should_cleanup():
            return self.cleanup_memory()
        return {'cleaned_items': 0, 'freed_mb': 0}

    def cleanup_memory(self) -> Dict[str, int]:
        """æ·±åº¦å†…å­˜æ¸…ç†"""
        cleaned_stats = {
            'memory_pools_cleaned': 0,
            'object_cache_cleaned': 0,
            'gc_collected': 0,
            'freed_mb': 0
        }

        # è®°å½•æ¸…ç†å‰çš„å†…å­˜ä½¿ç”¨
        before_memory = self.get_memory_usage()['rss_mb']

        with self._lock:
            # æ¸…ç†å†…å­˜æ± 
            for pool_name in list(self._memory_pools.keys()):
                pool = self._memory_pools[pool_name]
                if len(pool) > 10:  # ä¿ç•™æœ€è¿‘çš„ 10 ä¸ªé¡¹ç›®
                    removed = len(pool) - 10
                    self._memory_pools[pool_name] = pool[-10:]
                    cleaned_stats['memory_pools_cleaned'] += removed

            # æ¸…ç†å¯¹è±¡ç¼“å­˜
            if len(self._object_cache) > 50:
                # ä¿ç•™æœ€è¿‘ä½¿ç”¨çš„ 50 ä¸ªå¯¹è±¡
                cache_items = list(self._object_cache.items())
                self._object_cache = dict(cache_items[-50:])
                cleaned_stats['object_cache_cleaned'] = len(cache_items) - 50

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        collected = gc.collect()
        cleaned_stats['gc_collected'] = collected

        # è®¡ç®—é‡Šæ”¾çš„å†…å­˜
        after_memory = self.get_memory_usage()['rss_mb']
        cleaned_stats['freed_mb'] = max(0, before_memory - after_memory)

        return cleaned_stats

    def get_memory_analysis(self) -> Dict[str, Any]:
        """è·å–å†…å­˜åˆ†ææŠ¥å‘Š"""
        current_usage = self.get_memory_usage()
        object_analysis = self._analyzer.get_object_memory_usage()
        leak_analysis = self._analyzer.analyze_memory_leaks()

        # è®¡ç®—å†…å­˜è¶‹åŠ¿
        memory_trend = self._calculate_memory_trend()

        return {
            'current_usage': current_usage,
            'object_analysis': object_analysis,
            'leak_analysis': leak_analysis,
            'memory_trend': memory_trend,
            'pool_stats': {
                'total_pools': len(self._memory_pools),
                'total_cached_objects': sum(len(pool) for pool in self._memory_pools.values()),
                'object_cache_size': len(self._object_cache)
            },
            'recommendations': self._generate_memory_recommendations(current_usage)
        }

    def _calculate_memory_trend(self) -> Dict[str, Any]:
        """è®¡ç®—å†…å­˜ä½¿ç”¨è¶‹åŠ¿"""
        if len(self._memory_history) < 3:
            return {'trend': 'insufficient_data', 'growth_rate': 0}

        recent_usage = [entry['rss_mb'] for entry in self._memory_history[-10:]]

        # ç®€å•çº¿æ€§è¶‹åŠ¿è®¡ç®—
        if len(recent_usage) >= 2:
            growth_rate = (recent_usage[-1] - recent_usage[0]) / len(recent_usage)

            if growth_rate > 5:
                trend = 'increasing_rapidly'
            elif growth_rate > 1:
                trend = 'increasing'
            elif growth_rate < -1:
                trend = 'decreasing'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
            growth_rate = 0

        return {
            'trend': trend,
            'growth_rate': growth_rate,
            'history_points': len(self._memory_history)
        }

    def _generate_memory_recommendations(self, usage: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå†…å­˜ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        rss_mb = usage.get('rss_mb', 0)

        if rss_mb > self.max_memory_mb * 0.9:
            recommendations.append("å†…å­˜ä½¿ç”¨æ¥è¿‘é™åˆ¶ï¼Œå»ºè®®ç«‹å³æ¸…ç†ç¼“å­˜")
        elif rss_mb > self.max_memory_mb * 0.7:
            recommendations.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®å®šæœŸæ¸…ç†")

        if usage.get('system_used_percent', 0) > 80:
            recommendations.append("ç³»ç»Ÿå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå»ºè®®å‡å°‘å¹¶å‘æ“ä½œ")

        if self._is_memory_growing_rapidly():
            recommendations.append("æ£€æµ‹åˆ°å†…å­˜å¿«é€Ÿå¢é•¿ï¼Œå¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼")

        if len(self._memory_pools) > 20:
            recommendations.append("å†…å­˜æ± è¿‡å¤šï¼Œå»ºè®®åˆå¹¶æˆ–æ¸…ç†")

        if not recommendations:
            recommendations.append("å†…å­˜ä½¿ç”¨æ­£å¸¸ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")

        return recommendations


# ================== çœŸæ­£çš„å¼‚æ­¥ I/O å¤„ç†å™¨ ==================
class AsyncIOProcessor:
    """çœŸæ­£çš„å¼‚æ­¥ I/O å¤„ç†å™¨ - v1.5.0 æ·±åº¦å¼‚æ­¥ä¼˜åŒ–"""

    def __init__(self, max_concurrent: int = 10):
        self.max_concurrent = max_concurrent
        self.semaphore = threading.Semaphore(max_concurrent)
        self._loop = None
        self._executor = None

    def _get_event_loop(self):
        """è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯"""
        try:
            import asyncio
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            return loop
        except ImportError:
            return None

    def _get_executor(self):
        """è·å–çº¿ç¨‹æ± æ‰§è¡Œå™¨"""
        if self._executor is None:
            self._executor = ThreadPoolExecutor(max_workers=self.max_concurrent)
        return self._executor

    async def async_directory_scan_native(self, base_path: Path, max_depth: int = 3) -> List[Path]:
        """åŸç”Ÿå¼‚æ­¥ç›®å½•æ‰«æ"""
        import asyncio

        folders = []
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def scan_directory(path: Path, depth: int):
            if depth >= max_depth:
                return

            async with semaphore:
                try:
                    # å¼‚æ­¥æ‰«æç›®å½•
                    entries = await asyncio.get_event_loop().run_in_executor(
                        self._get_executor(),
                        lambda: list(os.scandir(path))
                    )

                    subdirs = []
                    for entry in entries:
                        if entry.is_dir(follow_symlinks=False):
                            folder_path = Path(entry.path)
                            folders.append(folder_path)
                            if depth + 1 < max_depth:
                                subdirs.append(folder_path)

                    # å¹¶å‘æ‰«æå­ç›®å½•
                    if subdirs:
                        tasks = [scan_directory(subdir, depth + 1) for subdir in subdirs]
                        await asyncio.gather(*tasks, return_exceptions=True)

                except (PermissionError, OSError):
                    pass

        await scan_directory(base_path, 0)
        return folders

    def async_directory_scan(self, base_path: Path, max_depth: int = 3) -> List[Path]:
        """å¼‚æ­¥ç›®å½•æ‰«æ - å…¼å®¹æ¥å£"""
        loop = self._get_event_loop()
        if loop is None:
            # å›é€€åˆ°çº¿ç¨‹æ± å®ç°
            return self._async_directory_scan_threaded(base_path, max_depth)

        try:
            import asyncio
            if loop.is_running():
                # å¦‚æœå¾ªç¯æ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨ run_in_executor
                future = asyncio.ensure_future(self.async_directory_scan_native(base_path, max_depth))
                return asyncio.run_coroutine_threadsafe(future, loop).result(timeout=30)
            else:
                # å¦‚æœå¾ªç¯æœªè¿è¡Œï¼Œç›´æ¥è¿è¡Œ
                return loop.run_until_complete(self.async_directory_scan_native(base_path, max_depth))
        except Exception:
            # å¼‚å¸¸æ—¶å›é€€åˆ°çº¿ç¨‹æ± å®ç°
            return self._async_directory_scan_threaded(base_path, max_depth)

    def _async_directory_scan_threaded(self, base_path: Path, max_depth: int = 3) -> List[Path]:
        """çº¿ç¨‹æ± ç‰ˆæœ¬çš„å¼‚æ­¥ç›®å½•æ‰«æ"""
        import queue
        import threading

        result_queue = queue.Queue()
        scan_queue = queue.Queue()
        scan_queue.put((base_path, 0))

        def worker():
            while True:
                try:
                    path, depth = scan_queue.get(timeout=1)
                    if depth >= max_depth:
                        scan_queue.task_done()
                        continue

                    try:
                        with os.scandir(path) as entries:
                            for entry in entries:
                                if entry.is_dir(follow_symlinks=False):
                                    folder_path = Path(entry.path)
                                    result_queue.put(folder_path)
                                    if depth + 1 < max_depth:
                                        scan_queue.put((folder_path, depth + 1))
                    except (PermissionError, OSError):
                        pass

                    scan_queue.task_done()
                except queue.Empty:
                    break

        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        threads = []
        for _ in range(min(4, self.max_concurrent)):
            t = threading.Thread(target=worker)
            t.daemon = True
            t.start()
            threads.append(t)

        # ç­‰å¾…å®Œæˆ
        scan_queue.join()

        # æ”¶é›†ç»“æœ
        folders = []
        while not result_queue.empty():
            folders.append(result_queue.get())

        return folders

    async def async_file_operations_native(self, operations: List[Tuple[str, Path, Any]]) -> List[Any]:
        """åŸç”Ÿå¼‚æ­¥æ–‡ä»¶æ“ä½œ"""
        import asyncio

        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def execute_operation(op_type: str, path: Path, params: Any) -> Any:
            async with semaphore:
                try:
                    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨å¤„ç† I/O æ“ä½œ
                    loop = asyncio.get_event_loop()

                    if op_type == 'size':
                        return await loop.run_in_executor(
                            self._get_executor(),
                            lambda: path.stat().st_size
                        )
                    elif op_type == 'exists':
                        return await loop.run_in_executor(
                            self._get_executor(),
                            lambda: path.exists()
                        )
                    elif op_type == 'mtime':
                        return await loop.run_in_executor(
                            self._get_executor(),
                            lambda: path.stat().st_mtime
                        )
                    elif op_type == 'hash':
                        return await self._async_calculate_hash(path, params or 'md5')
                    elif op_type == 'read':
                        return await self._async_read_file(path, params)
                    else:
                        return None
                except (OSError, IOError):
                    return None

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ“ä½œ
        tasks = [execute_operation(op_type, path, params) for op_type, path, params in operations]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸ç»“æœ
        return [result if not isinstance(result, Exception) else None for result in results]

    async def _async_calculate_hash(self, file_path: Path, algorithm: str = 'md5') -> str:
        """å¼‚æ­¥è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        import hashlib
        import asyncio

        hash_obj = hashlib.new(algorithm)
        chunk_size = 64 * 1024  # 64KB chunks for async operations

        try:
            loop = asyncio.get_event_loop()

            # å¼‚æ­¥è¯»å–æ–‡ä»¶
            def read_chunk(f, size):
                return f.read(size)

            with open(file_path, 'rb') as f:
                while True:
                    chunk = await loop.run_in_executor(
                        self._get_executor(),
                        read_chunk, f, chunk_size
                    )

                    if not chunk:
                        break

                    hash_obj.update(chunk)

                    # è®©å‡ºæ§åˆ¶æƒï¼Œå…è®¸å…¶ä»–åç¨‹è¿è¡Œ
                    await asyncio.sleep(0)

            return hash_obj.hexdigest()

        except (OSError, IOError):
            return ""

    async def _async_read_file(self, file_path: Path, max_size: int = None) -> bytes:
        """å¼‚æ­¥è¯»å–æ–‡ä»¶"""
        import asyncio

        try:
            loop = asyncio.get_event_loop()

            def read_file():
                with open(file_path, 'rb') as f:
                    if max_size:
                        return f.read(max_size)
                    else:
                        return f.read()

            return await loop.run_in_executor(self._get_executor(), read_file)

        except (OSError, IOError):
            return b""

    def async_file_operations(self, operations: List[Tuple[str, Path, Any]]) -> List[Any]:
        """å¼‚æ­¥æ–‡ä»¶æ“ä½œ - å…¼å®¹æ¥å£"""
        loop = self._get_event_loop()
        if loop is None:
            # å›é€€åˆ°çº¿ç¨‹æ± å®ç°
            return self._async_file_operations_threaded(operations)

        try:
            import asyncio
            if loop.is_running():
                # å¦‚æœå¾ªç¯æ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨ run_in_executor
                future = asyncio.ensure_future(self.async_file_operations_native(operations))
                return asyncio.run_coroutine_threadsafe(future, loop).result(timeout=60)
            else:
                # å¦‚æœå¾ªç¯æœªè¿è¡Œï¼Œç›´æ¥è¿è¡Œ
                return loop.run_until_complete(self.async_file_operations_native(operations))
        except Exception:
            # å¼‚å¸¸æ—¶å›é€€åˆ°çº¿ç¨‹æ± å®ç°
            return self._async_file_operations_threaded(operations)

    def _async_file_operations_threaded(self, operations: List[Tuple[str, Path, Any]]) -> List[Any]:
        """çº¿ç¨‹æ± ç‰ˆæœ¬çš„å¼‚æ­¥æ–‡ä»¶æ“ä½œ"""
        results = []

        def execute_operation(op_type: str, path: Path, params: Any) -> Any:
            with self.semaphore:
                try:
                    if op_type == 'size':
                        return path.stat().st_size
                    elif op_type == 'exists':
                        return path.exists()
                    elif op_type == 'mtime':
                        return path.stat().st_mtime
                    else:
                        return None
                except (OSError, IOError):
                    return None

        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            futures = [
                executor.submit(execute_operation, op_type, path, params)
                for op_type, path, params in operations
            ]

            for future in as_completed(futures):
                results.append(future.result())

        return results

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self._executor:
            self._executor.shutdown(wait=True)
            self._executor = None


# ================== å¼‚æ­¥æ–‡ä»¶å¤„ç†å™¨ ==================
class AsyncFileProcessor:
    """å¼‚æ­¥æ–‡ä»¶å¤„ç†å™¨ - ä¸“é—¨å¤„ç†æ–‡ä»¶ç›¸å…³çš„å¼‚æ­¥æ“ä½œ"""

    def __init__(self, max_concurrent: int = 8, chunk_size: int = 64 * 1024):
        self.max_concurrent = max_concurrent
        self.chunk_size = chunk_size
        self.async_io = AsyncIOProcessor(max_concurrent)

    async def async_batch_file_stats(self, file_paths: List[Path]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¼‚æ­¥è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        import asyncio

        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def get_file_stats(file_path: Path) -> Dict[str, Any]:
            async with semaphore:
                try:
                    loop = asyncio.get_event_loop()

                    # å¼‚æ­¥è·å–æ–‡ä»¶çŠ¶æ€
                    stat_info = await loop.run_in_executor(
                        None, lambda: file_path.stat()
                    )

                    return {
                        'path': str(file_path),
                        'size': stat_info.st_size,
                        'mtime': stat_info.st_mtime,
                        'is_file': file_path.is_file(),
                        'exists': True
                    }
                except (OSError, IOError):
                    return {
                        'path': str(file_path),
                        'size': 0,
                        'mtime': 0,
                        'is_file': False,
                        'exists': False
                    }

        # å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡ä»¶
        tasks = [get_file_stats(path) for path in file_paths]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        return [result for result in results if isinstance(result, dict)]

    async def async_directory_tree_scan(self, base_path: Path,
                                      max_depth: int = 3,
                                      include_files: bool = True) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰«æç›®å½•æ ‘"""
        import asyncio

        result = {
            'directories': [],
            'files': [],
            'total_size': 0,
            'file_count': 0,
            'dir_count': 0,
            'errors': []
        }

        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def scan_directory(path: Path, depth: int):
            if depth >= max_depth:
                return

            async with semaphore:
                try:
                    loop = asyncio.get_event_loop()

                    # å¼‚æ­¥æ‰«æç›®å½•
                    entries = await loop.run_in_executor(
                        None, lambda: list(os.scandir(path))
                    )

                    subdirs = []
                    files = []

                    for entry in entries:
                        if entry.is_dir(follow_symlinks=False):
                            dir_path = Path(entry.path)
                            result['directories'].append(str(dir_path))
                            result['dir_count'] += 1

                            if depth + 1 < max_depth:
                                subdirs.append(dir_path)

                        elif entry.is_file(follow_symlinks=False) and include_files:
                            file_path = Path(entry.path)
                            try:
                                file_size = entry.stat().st_size
                                files.append({
                                    'path': str(file_path),
                                    'size': file_size
                                })
                                result['total_size'] += file_size
                                result['file_count'] += 1
                            except (OSError, IOError):
                                result['errors'].append(f"æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯: {file_path}")

                    # æ‰¹é‡æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                    if files:
                        result['files'].extend(files)

                    # å¹¶å‘æ‰«æå­ç›®å½•
                    if subdirs:
                        tasks = [scan_directory(subdir, depth + 1) for subdir in subdirs]
                        await asyncio.gather(*tasks, return_exceptions=True)

                except (PermissionError, OSError) as e:
                    result['errors'].append(f"æ‰«æç›®å½•å¤±è´¥ {path}: {e}")

        await scan_directory(base_path, 0)
        return result

    async def async_file_hash_batch(self, file_paths: List[Path],
                                  algorithm: str = 'md5',
                                  progress_callback=None) -> Dict[str, str]:
        """æ‰¹é‡å¼‚æ­¥è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        import asyncio
        import hashlib

        semaphore = asyncio.Semaphore(self.max_concurrent)
        results = {}
        completed = 0
        total = len(file_paths)

        async def calculate_hash(file_path: Path) -> Tuple[str, str]:
            nonlocal completed

            async with semaphore:
                try:
                    hash_obj = hashlib.new(algorithm)
                    loop = asyncio.get_event_loop()

                    def read_and_hash():
                        with open(file_path, 'rb') as f:
                            while chunk := f.read(self.chunk_size):
                                hash_obj.update(chunk)
                        return hash_obj.hexdigest()

                    file_hash = await loop.run_in_executor(None, read_and_hash)

                    completed += 1
                    if progress_callback:
                        progress_callback(completed / total, f"è®¡ç®—å“ˆå¸Œ: {file_path.name}")

                    return str(file_path), file_hash

                except (OSError, IOError):
                    completed += 1
                    return str(file_path), ""

        # å¹¶å‘è®¡ç®—æ‰€æœ‰æ–‡ä»¶å“ˆå¸Œ
        tasks = [calculate_hash(path) for path in file_paths]
        hash_results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ•´ç†ç»“æœ
        for result in hash_results:
            if isinstance(result, tuple) and len(result) == 2:
                path, file_hash = result
                results[path] = file_hash

        return results

    def run_async_batch_file_stats(self, file_paths: List[Path]) -> List[Dict[str, Any]]:
        """è¿è¡Œæ‰¹é‡æ–‡ä»¶ç»Ÿè®¡ - åŒæ­¥æ¥å£"""
        loop = self.async_io._get_event_loop()
        if loop is None:
            # å›é€€åˆ°åŒæ­¥å®ç°
            return self._sync_batch_file_stats(file_paths)

        try:
            import asyncio
            if loop.is_running():
                future = asyncio.ensure_future(self.async_batch_file_stats(file_paths))
                return asyncio.run_coroutine_threadsafe(future, loop).result(timeout=60)
            else:
                return loop.run_until_complete(self.async_batch_file_stats(file_paths))
        except Exception:
            return self._sync_batch_file_stats(file_paths)

    def _sync_batch_file_stats(self, file_paths: List[Path]) -> List[Dict[str, Any]]:
        """åŒæ­¥ç‰ˆæœ¬çš„æ‰¹é‡æ–‡ä»¶ç»Ÿè®¡"""
        results = []
        for file_path in file_paths:
            try:
                stat_info = file_path.stat()
                results.append({
                    'path': str(file_path),
                    'size': stat_info.st_size,
                    'mtime': stat_info.st_mtime,
                    'is_file': file_path.is_file(),
                    'exists': True
                })
            except (OSError, IOError):
                results.append({
                    'path': str(file_path),
                    'size': 0,
                    'mtime': 0,
                    'is_file': False,
                    'exists': False
                })
        return results


# ================== å†…å­˜æ„ŸçŸ¥æµå¼å¤„ç†å™¨ ==================
class StreamFileProcessor:
    """å†…å­˜æ„ŸçŸ¥æµå¼æ–‡ä»¶å¤„ç†å™¨ - æ™ºèƒ½å¤„ç†å¤§æ–‡ä»¶é¿å…å†…å­˜æº¢å‡º"""

    def __init__(self, chunk_size: int = 1024 * 1024, memory_manager: 'MemoryManager' = None):
        self.base_chunk_size = chunk_size
        self.memory_manager = memory_manager
        self._adaptive_chunk_size = chunk_size
        self._processed_bytes = 0

    def _get_adaptive_chunk_size(self) -> int:
        """æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè‡ªé€‚åº”è°ƒæ•´å—å¤§å°"""
        if not self.memory_manager:
            return self.base_chunk_size

        memory_info = self.memory_manager.get_memory_usage()
        memory_usage_percent = memory_info.get('rss_mb', 0) / self.memory_manager.max_memory_mb

        if memory_usage_percent > 0.8:
            # å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå‡å°å—å¤§å°
            self._adaptive_chunk_size = max(64 * 1024, self.base_chunk_size // 4)
        elif memory_usage_percent > 0.6:
            # å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œé€‚åº¦å‡å°å—å¤§å°
            self._adaptive_chunk_size = max(256 * 1024, self.base_chunk_size // 2)
        else:
            # å†…å­˜ä½¿ç”¨æ­£å¸¸ï¼Œä½¿ç”¨æ ‡å‡†å—å¤§å°
            self._adaptive_chunk_size = self.base_chunk_size

        return self._adaptive_chunk_size

    def calculate_file_hash(self, file_path: Path, algorithm: str = 'md5',
                          progress_callback=None) -> str:
        """å†…å­˜ä¼˜åŒ–çš„æµå¼æ–‡ä»¶å“ˆå¸Œè®¡ç®—"""
        import hashlib

        hash_obj = hashlib.new(algorithm)
        processed_bytes = 0

        try:
            file_size = file_path.stat().st_size

            with open(file_path, 'rb') as f:
                while True:
                    # åŠ¨æ€è°ƒæ•´å—å¤§å°
                    chunk_size = self._get_adaptive_chunk_size()
                    chunk = f.read(chunk_size)

                    if not chunk:
                        break

                    hash_obj.update(chunk)
                    processed_bytes += len(chunk)

                    # å®šæœŸæ£€æŸ¥å†…å­˜å¹¶æ¸…ç†
                    if processed_bytes % (10 * 1024 * 1024) == 0:  # æ¯ 10MB æ£€æŸ¥ä¸€æ¬¡
                        if self.memory_manager and self.memory_manager.should_cleanup():
                            self.memory_manager.cleanup_if_needed()

                    # è¿›åº¦å›è°ƒ
                    if progress_callback and file_size > 0:
                        progress = processed_bytes / file_size
                        progress_callback(progress)

            return hash_obj.hexdigest()

        except (OSError, IOError) as e:
            logger.warning(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return ""

    def get_file_size_stream(self, file_path: Path) -> int:
        """å®‰å…¨è·å–æ–‡ä»¶å¤§å°"""
        try:
            return file_path.stat().st_size
        except (OSError, IOError):
            return 0

    def copy_file_stream(self, src: Path, dst: Path, progress_callback=None) -> bool:
        """å†…å­˜ä¼˜åŒ–çš„æµå¼æ–‡ä»¶å¤åˆ¶"""
        try:
            src_size = src.stat().st_size
            copied_bytes = 0

            with open(src, 'rb') as src_file, open(dst, 'wb') as dst_file:
                while True:
                    chunk_size = self._get_adaptive_chunk_size()
                    chunk = src_file.read(chunk_size)

                    if not chunk:
                        break

                    dst_file.write(chunk)
                    copied_bytes += len(chunk)

                    # å†…å­˜æ£€æŸ¥å’Œæ¸…ç†
                    if copied_bytes % (20 * 1024 * 1024) == 0:  # æ¯ 20MB æ£€æŸ¥ä¸€æ¬¡
                        if self.memory_manager and self.memory_manager.should_cleanup():
                            self.memory_manager.cleanup_if_needed()

                    # è¿›åº¦å›è°ƒ
                    if progress_callback and src_size > 0:
                        progress = copied_bytes / src_size
                        progress_callback(progress)

            return True

        except (OSError, IOError) as e:
            logger.warning(f"æµå¼å¤åˆ¶æ–‡ä»¶å¤±è´¥: {src} -> {dst}, é”™è¯¯: {e}")
            return False

    def process_large_directory(self, directory: Path,
                              operation: str = 'size') -> Dict[str, Any]:
        """å†…å­˜ä¼˜åŒ–çš„å¤§ç›®å½•å¤„ç†"""
        results = {
            'total_size': 0,
            'file_count': 0,
            'processed_files': [],
            'errors': []
        }

        try:
            for file_path in directory.rglob('*'):
                if file_path.is_file():
                    try:
                        if operation == 'size':
                            file_size = self.get_file_size_stream(file_path)
                            results['total_size'] += file_size
                            results['file_count'] += 1

                            # è®°å½•å¤§æ–‡ä»¶
                            if file_size > 100 * 1024 * 1024:  # å¤§äº 100MB
                                results['processed_files'].append({
                                    'path': str(file_path),
                                    'size': file_size
                                })

                        # å®šæœŸå†…å­˜æ£€æŸ¥
                        if results['file_count'] % 1000 == 0:
                            if self.memory_manager and self.memory_manager.should_cleanup():
                                cleaned = self.memory_manager.cleanup_if_needed()
                                if cleaned.get('freed_mb', 0) > 0:
                                    logger.info(f"å¤„ç†å¤§ç›®å½•æ—¶æ¸…ç†å†…å­˜: {cleaned['freed_mb']:.1f}MB")

                    except (OSError, IOError) as e:
                        results['errors'].append(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        except Exception as e:
            results['errors'].append(f"ç›®å½•å¤„ç†å¤±è´¥: {e}")

        return results

    def async_calculate_directory_size(self, path: Path) -> int:
        """å¼‚æ­¥è®¡ç®—ç›®å½•å¤§å°"""
        total_size = 0
        file_operations = []

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶æ“ä½œ
        try:
            for file_path in path.rglob('*'):
                if file_path.is_file():
                    file_operations.append(('size', file_path, None))
        except (OSError, PermissionError):
            return 0

        # å¼‚æ­¥æ‰§è¡Œæ–‡ä»¶å¤§å°è®¡ç®—
        if file_operations:
            async_processor = AsyncIOProcessor(max_concurrent=8)
            sizes = async_processor.async_file_operations(file_operations)
            total_size = sum(size for size in sizes if size is not None)

        return total_size


# ================== é«˜æ€§èƒ½ç›¸ä¼¼åº¦è®¡ç®— ==================
class FastSimilarityCalculator:
    """é«˜æ€§èƒ½ç›¸ä¼¼åº¦è®¡ç®—å™¨"""

    @staticmethod
    def jaccard_similarity(set_a: Set[str], set_b: Set[str]) -> float:
        """Jaccard ç›¸ä¼¼åº¦è®¡ç®— - æ¯” SequenceMatcher æ›´å¿«"""
        if not set_a or not set_b:
            return 0.0

        intersection = len(set_a.intersection(set_b))
        union = len(set_a.union(set_b))

        return intersection / union if union > 0 else 0.0

    @staticmethod
    def word_overlap_ratio(search_words: Set[str], target_words: Set[str]) -> float:
        """è¯æ±‡é‡å æ¯”ä¾‹"""
        if not search_words:
            return 0.0

        overlap = len(search_words.intersection(target_words))
        return overlap / len(search_words)

    @staticmethod
    def substring_bonus(search_str: str, target_str: str) -> float:
        """å­å­—ç¬¦ä¸²åŒ¹é…å¥–åŠ±"""
        if search_str in target_str:
            return 0.3
        elif target_str in search_str:
            return 0.2
        return 0.0


# ================== æ–‡ä»¶åŒ¹é…å™¨ ==================
class FileMatcher:
    """æ–‡ä»¶åŒ¹é…å™¨ - v1.5.0 é«˜æ€§èƒ½æœç´¢ä¼˜åŒ–ç‰ˆæœ¬"""

    VIDEO_EXTENSIONS = {
        '.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv',
        '.webm', '.m4v', '.3gp', '.ogv', '.ts', '.m2ts',
        '.mpg', '.mpeg', '.rm', '.rmvb', '.asf', '.divx'
    }

    STOP_WORDS = {
        'the', 'and', 'of', 'to', 'in', 'a', 'an', 'is', 'are',
        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had'
    }

    SEPARATORS = ['.', '_', '-', ':', '|', '\\', '/', '+', '(', ')', '[', ']']

    def __init__(self, base_directory: str, enable_cache: bool = True,
                 cache_duration: int = 3600, min_score: float = 0.6,
                 max_workers: int = 4):
        self.base_directory = Path(base_directory)
        self.min_score = min_score
        self.max_workers = max_workers
        self.cache = SearchCache(cache_duration) if enable_cache else None
        self.folder_info_cache = SearchCache(cache_duration) if enable_cache else None
        self.performance_monitor = PerformanceMonitor()

        # v1.5.0 æ–°å¢ï¼šæ™ºèƒ½ç´¢å¼•ç¼“å­˜ã€å†…å­˜ç®¡ç†å’Œå¼‚æ­¥ I/O
        self.smart_index = SmartIndexCache(cache_duration)
        self.similarity_calc = FastSimilarityCalculator()
        self.memory_manager = MemoryManager(max_memory_mb=256)  # é™åˆ¶ 256MB
        self.stream_processor = StreamFileProcessor(memory_manager=self.memory_manager)
        self.async_processor = AsyncIOProcessor(max_concurrent=6)
        self.async_file_processor = AsyncFileProcessor(max_concurrent=4)
        self._compiled_patterns = self._compile_quality_patterns()

        if not self.base_directory.exists():
            logger.warning(f"åŸºç¡€ç›®å½•ä¸å­˜åœ¨: {self.base_directory}")

    def _compile_quality_patterns(self) -> List:
        """é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        import re
        quality_patterns = [
            r'\b(720p|1080p|4k|uhd|hd|sd|bluray|bdrip|webrip|hdtv)\b',
            r'\b(x264|x265|h264|h265|hevc)\b',
            r'\b(aac|ac3|dts|mp3)\b'
        ]
        return [re.compile(pattern, re.IGNORECASE) for pattern in quality_patterns]

    def _generate_cache_key(self, search_name: str) -> str:
        key_data = f"{search_name}:{self.base_directory}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _normalize_string(self, text: str) -> str:
        """é«˜æ€§èƒ½å­—ç¬¦ä¸²æ ‡å‡†åŒ– - v1.5.0 ä¼˜åŒ–ç‰ˆæœ¬"""
        if not text:
            return ""

        # ç¼“å­˜æ ‡å‡†åŒ–ç»“æœ
        cache_key = f"normalize:{text}"
        if self.cache:
            cached_result = self.cache.get(cache_key)
            if cached_result is not None:
                return cached_result

        text = text.lower()

        # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼
        text = re.sub(r'\b(19|20)\d{2}\b', '', text)

        # ä½¿ç”¨é¢„ç¼–è¯‘çš„è´¨é‡æ ‡è¯†æ¨¡å¼
        for pattern in self._compiled_patterns:
            text = pattern.sub('', text)

        # æ‰¹é‡æ›¿æ¢åˆ†éš”ç¬¦ï¼ˆæ›´é«˜æ•ˆï¼‰
        for sep in self.SEPARATORS:
            text = text.replace(sep, ' ')

        text = re.sub(r'\s+', ' ', text).strip()

        # ä¼˜åŒ–åœç”¨è¯ç§»é™¤
        words = text.split()
        if len(words) > 3:
            # ä½¿ç”¨é›†åˆæ“ä½œï¼Œæ›´é«˜æ•ˆ
            word_set = set(words)
            filtered_set = word_set - self.STOP_WORDS
            if filtered_set:
                # ä¿æŒåŸå§‹é¡ºåº
                words = [word for word in words if word in filtered_set]

        result = ' '.join(words)

        # ç¼“å­˜ç»“æœ
        if self.cache:
            self.cache.set(cache_key, result)

        return result

    def similarity(self, a: str, b: str) -> float:
        """é«˜æ€§èƒ½ç›¸ä¼¼åº¦è®¡ç®— - v1.5.0 ä¼˜åŒ–ç‰ˆæœ¬"""
        # ç¼“å­˜ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ
        cache_key = f"sim:{a}:{b}"
        if self.cache:
            cached_score = self.cache.get(cache_key)
            if cached_score is not None:
                return cached_score

        a_normalized = self._normalize_string(a)
        b_normalized = self._normalize_string(b)

        # å¿«é€Ÿå®Œå…¨åŒ¹é…æ£€æŸ¥
        if a_normalized == b_normalized:
            score = 1.0
        else:
            # ä½¿ç”¨æ›´å¿«çš„ç®—æ³•ç»„åˆ
            a_words = set(a_normalized.split())
            b_words = set(b_normalized.split())

            # 1. Jaccard ç›¸ä¼¼åº¦ï¼ˆæ¯” SequenceMatcher æ›´å¿«ï¼‰
            jaccard_score = self.similarity_calc.jaccard_similarity(a_words, b_words)

            # 2. è¯æ±‡é‡å æ¯”ä¾‹
            overlap_ratio = self.similarity_calc.word_overlap_ratio(a_words, b_words)

            # 3. å­å­—ç¬¦ä¸²åŒ¹é…å¥–åŠ±
            substring_bonus = self.similarity_calc.substring_bonus(a_normalized, b_normalized)

            # ç»„åˆå¾—åˆ†
            score = jaccard_score * 0.6 + overlap_ratio * 0.3 + substring_bonus

            # å¦‚æœè¯æ±‡é‡å åº¦å¾ˆé«˜ï¼Œç»™äºˆé¢å¤–å¥–åŠ±
            if overlap_ratio >= 0.8:
                score = max(score, 0.9)
            elif overlap_ratio >= 0.6:
                score = max(score, 0.8)

        score = min(1.0, score)

        # ç¼“å­˜ç»“æœ
        if self.cache:
            self.cache.set(cache_key, score)

        return score

    def get_all_folders(self, max_depth: int = 3) -> List[Path]:
        """è·å–åŸºç¡€ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹ - å¼‚æ­¥I/Oä¼˜åŒ–ç‰ˆæœ¬"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"all_folders:{self.base_directory}:{max_depth}"
        if self.cache:
            cached_folders = self.cache.get(cache_key)
            if cached_folders is not None:
                return cached_folders

        self.performance_monitor.start_timer('folder_scanning')
        folders = []

        if not self.base_directory.exists():
            return folders

        try:
            # å°è¯•ä½¿ç”¨å¼‚æ­¥ç›®å½•æ‰«æ
            async_folders = self._try_async_folder_scan(max_depth)
            if async_folders is not None:
                folders = async_folders
            else:
                # å›é€€åˆ°åŒæ­¥æ‰«æ
                folders = self._sync_folder_scan(max_depth)

            # ç¼“å­˜ç»“æœï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
            if self.cache and not self.memory_manager.should_cleanup():
                self.cache.set(cache_key, folders)

        finally:
            scan_duration = self.performance_monitor.end_timer('folder_scanning')
            memory_info = self.memory_manager.get_memory_usage()

            if scan_duration > 3.0:
                logger.warning(f"æ–‡ä»¶å¤¹æ‰«æè€—æ—¶è¾ƒé•¿: {scan_duration:.2f}s, æ‰¾åˆ° {len(folders)} ä¸ªæ–‡ä»¶å¤¹")

            print(f"  ğŸ“Š å†…å­˜ä½¿ç”¨: {memory_info['rss_mb']:.1f}MB, æ‰¾åˆ° {len(folders)} ä¸ªæ–‡ä»¶å¤¹")

        return folders

    def _try_async_folder_scan(self, max_depth: int) -> Optional[List[Path]]:
        """å°è¯•å¼‚æ­¥æ–‡ä»¶å¤¹æ‰«æ"""
        try:
            # ä½¿ç”¨å¼‚æ­¥ç›®å½•æ‰«æ
            async_folders = self.async_processor.async_directory_scan(self.base_directory, max_depth)
            print(f"  âš¡ å¼‚æ­¥æ‰«æå®Œæˆ: æ‰¾åˆ° {len(async_folders)} ä¸ªæ–‡ä»¶å¤¹")
            return async_folders
        except Exception as e:
            logger.debug(f"å¼‚æ­¥æ‰«æå¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ¨¡å¼: {e}")
            return None

    def _sync_folder_scan(self, max_depth: int) -> List[Path]:
        """åŒæ­¥æ–‡ä»¶å¤¹æ‰«æ"""
        folders = []

        def _scan_directory_memory_optimized(path: Path, current_depth: int = 0):
            if current_depth >= max_depth:
                return

            # å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨
            if len(folders) % 1000 == 0:
                cleaned = self.memory_manager.cleanup_if_needed()
                if cleaned.get('freed_mb', 0) > 0:
                    print(f"  ğŸ§¹ å†…å­˜æ¸…ç†: é‡Šæ”¾ {cleaned['freed_mb']:.1f}MB, {cleaned.get('memory_pools_cleaned', 0)} ä¸ªç¼“å­˜é¡¹")

            try:
                with os.scandir(path) as entries:
                    batch_folders = []

                    for entry in entries:
                        if entry.is_dir(follow_symlinks=False):
                            folder_path = Path(entry.path)
                            batch_folders.append(folder_path)

                            # æ‰¹é‡æ·»åŠ ï¼Œå‡å°‘å†…å­˜åˆ†é…
                            if len(batch_folders) >= 100:
                                folders.extend(batch_folders)
                                batch_folders.clear()

                    # æ·»åŠ å‰©ä½™çš„æ–‡ä»¶å¤¹
                    if batch_folders:
                        folders.extend(batch_folders)

                    # é€’å½’æ‰«æå­ç›®å½•
                    for entry in entries:
                        if entry.is_dir(follow_symlinks=False):
                            _scan_directory_memory_optimized(Path(entry.path), current_depth + 1)

            except (PermissionError, OSError):
                pass

        _scan_directory_memory_optimized(self.base_directory)
        print(f"  ğŸ”„ åŒæ­¥æ‰«æå®Œæˆ: æ‰¾åˆ° {len(folders)} ä¸ªæ–‡ä»¶å¤¹")
        return folders

    def fuzzy_search(self, search_name: str, max_results: int = 10) -> List[Tuple[str, float]]:
        """æ™ºèƒ½æ¨¡ç³Šæœç´¢ - v1.5.0 é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        self.performance_monitor.start_timer('fuzzy_search')

        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(search_name)
            if self.cache:
                cached_result = self.cache.get(cache_key)
                if cached_result is not None:
                    return cached_result[:max_results]

            all_folders = self.get_all_folders()
            if not all_folders:
                return []

            # é¢„å¤„ç†æœç´¢åç§°
            normalized_search = self._normalize_string(search_name)
            search_words = set(normalized_search.split())

            # æ„å»ºæˆ–æ›´æ–°æ™ºèƒ½ç´¢å¼•
            if self.smart_index.is_expired():
                self.smart_index.build_index(all_folders, self._normalize_string)

            # ä½¿ç”¨æ™ºèƒ½ç´¢å¼•è¿›è¡Œé¢„ç­›é€‰
            candidate_folders = self.smart_index.get_candidate_folders(search_words)

            # å¦‚æœé¢„ç­›é€‰ç»“æœå¤ªå°‘ï¼Œå›é€€åˆ°å…¨é‡æœç´¢
            if len(candidate_folders) < max_results * 2:
                candidate_paths = all_folders
                print(f"  ğŸ” é¢„ç­›é€‰ç»“æœè¾ƒå°‘({len(candidate_folders)})ï¼Œä½¿ç”¨å…¨é‡æœç´¢")
            else:
                candidate_paths = [Path(path) for path in candidate_folders]
                print(f"  ğŸ¯ æ™ºèƒ½é¢„ç­›é€‰: {len(all_folders)} â†’ {len(candidate_paths)} ä¸ªå€™é€‰")

            matches = []

            def process_folder_fast(folder_path: Path) -> Optional[Tuple[str, float]]:
                """å¿«é€Ÿæ–‡ä»¶å¤¹å¤„ç†"""
                try:
                    folder_name = folder_path.name
                    similarity_score = self.similarity(search_name, folder_name)

                    if similarity_score >= self.min_score:
                        return (str(folder_path), similarity_score)
                    return None
                except Exception:
                    return None

            # æ™ºèƒ½å¹¶å‘ç­–ç•¥
            folder_count = len(candidate_paths)
            if folder_count <= 50:
                # å°‘é‡æ–‡ä»¶å¤¹ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†
                for folder in candidate_paths:
                    result = process_folder_fast(folder)
                    if result:
                        matches.append(result)
            else:
                # å¤§é‡æ–‡ä»¶å¤¹ï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†
                batch_size = min(500, folder_count)

                for i in range(0, folder_count, batch_size):
                    batch_folders = candidate_paths[i:i + batch_size]

                    with ThreadPoolExecutor(max_workers=min(self.max_workers, 4)) as executor:
                        future_to_folder = {
                            executor.submit(process_folder_fast, folder): folder
                            for folder in batch_folders
                        }

                        for future in as_completed(future_to_folder):
                            result = future.result()
                            if result:
                                matches.append(result)

            # æ™ºèƒ½æ’åºï¼šç›¸ä¼¼åº¦ + è·¯å¾„é•¿åº¦ï¼ˆæ›´çŸ­çš„è·¯å¾„ä¼˜å…ˆï¼‰
            matches.sort(key=lambda x: (x[1], -len(x[0])), reverse=True)

            # ç¼“å­˜ç»“æœ
            if self.cache:
                self.cache.set(cache_key, matches)

            return matches[:max_results]

        finally:
            search_duration = self.performance_monitor.end_timer('fuzzy_search')
            matches_count = len(matches) if 'matches' in locals() else 0
            print(f"  ğŸ” æœç´¢è€—æ—¶: {search_duration:.3f}s, æ‰¾åˆ° {matches_count} ä¸ªåŒ¹é…é¡¹")

    def get_folder_info(self, folder_path: str) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶å¤¹è¯¦ç»†ä¿¡æ¯ - å¸¦ç¼“å­˜ä¼˜åŒ–"""
        if not os.path.exists(folder_path):
            return {'exists': False}

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"folder_info:{folder_path}"
        if self.folder_info_cache:
            cached_info = self.folder_info_cache.get(cache_key)
            if cached_info is not None:
                return cached_info

        self.performance_monitor.start_timer('folder_info_calculation')

        try:
            total_files = 0
            total_size = 0

            try:
                # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•è®¡ç®—æ–‡ä»¶ä¿¡æ¯
                path_obj = Path(folder_path)
                for file_path in path_obj.rglob('*'):
                    if file_path.is_file():
                        total_files += 1
                        try:
                            total_size += file_path.stat().st_size
                        except (OSError, IOError):
                            pass
            except PermissionError:
                result = {'exists': True, 'readable': False}
                if self.folder_info_cache:
                    self.folder_info_cache.set(cache_key, result)
                return result

            size_str = self.format_size(total_size)

            result = {
                'exists': True,
                'readable': True,
                'total_files': total_files,
                'total_size': total_size,
                'size_str': size_str
            }

            # ç¼“å­˜ç»“æœ
            if self.folder_info_cache:
                self.folder_info_cache.set(cache_key, result)

            return result

        finally:
            self.performance_monitor.end_timer('folder_info_calculation')

    def format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    def is_video_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
        return Path(filename).suffix.lower() in self.VIDEO_EXTENSIONS

    def match_folders(self, search_name: str) -> List[Dict[str, Any]]:
        """æœç´¢å¹¶è¿”å›åŒ¹é…çš„æ–‡ä»¶å¤¹ä¿¡æ¯"""
        matches = self.fuzzy_search(search_name)
        result = []

        for folder_path, score in matches:
            folder_info = self.get_folder_info(folder_path)
            if folder_info['exists']:
                episode_info = self.extract_episode_info_simple(folder_path)
                season_info = episode_info.get('season_info', '')
                total_episodes = episode_info.get('total_episodes', 0)

                result.append({
                    'path': folder_path,
                    'name': os.path.basename(folder_path),
                    'score': int(score * 100),
                    'file_count': folder_info.get('total_files', 0),
                    'size': folder_info.get('size_str', 'æœªçŸ¥'),
                    'readable': folder_info.get('readable', True),
                    'episodes': season_info,
                    'video_count': total_episodes
                })

        return result

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æœç´¢æ€§èƒ½ç»Ÿè®¡ - v1.5.0 å¢å¼ºç‰ˆ"""
        stats = self.performance_monitor.get_all_stats()
        memory_info = self.memory_manager.get_memory_usage()

        # è®¡ç®—æœç´¢æ•ˆç‡æŒ‡æ ‡
        search_stats = stats.get('fuzzy_search', {})
        folder_scan_stats = stats.get('folder_scanning', {})

        return {
            'search_performance': {
                'average_search_time': search_stats.get('average', 0),
                'total_searches': search_stats.get('count', 0),
                'fastest_search': search_stats.get('min', 0),
                'slowest_search': search_stats.get('max', 0)
            },
            'folder_scanning': {
                'average_scan_time': folder_scan_stats.get('average', 0),
                'total_scans': folder_scan_stats.get('count', 0)
            },
            'memory_usage': memory_info,
            'cache_performance': {
                'smart_index_expired': self.smart_index.is_expired(),
                'cache_enabled': self.cache is not None
            },
            'optimization_level': self._calculate_optimization_level(search_stats, memory_info)
        }

    def _calculate_optimization_level(self, search_stats: Dict, memory_info: Dict) -> str:
        """è®¡ç®—ä¼˜åŒ–ç­‰çº§"""
        avg_search_time = search_stats.get('average', 0)
        memory_usage = memory_info.get('rss_mb', 0)

        if avg_search_time < 0.5 and memory_usage < 200:
            return "ä¼˜ç§€ (A+)"
        elif avg_search_time < 1.0 and memory_usage < 300:
            return "è‰¯å¥½ (B+)"
        elif avg_search_time < 2.0 and memory_usage < 400:
            return "ä¸€èˆ¬ (C+)"
        else:
            return "éœ€è¦ä¼˜åŒ– (D)"

    def cleanup_resources(self) -> Dict[str, int]:
        """æ¸…ç†èµ„æº"""
        cleaned_stats = {}

        # æ¸…ç†å†…å­˜
        cleaned_stats['memory_items'] = self.memory_manager.cleanup_memory()

        # æ¸…ç†ç¼“å­˜
        if self.cache:
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç¼“å­˜æ¸…ç†é€»è¾‘
            cleaned_stats['cache_items'] = 0

        # é‡å»ºç´¢å¼•
        if self.smart_index.is_expired():
            cleaned_stats['index_rebuilt'] = 1
        else:
            cleaned_stats['index_rebuilt'] = 0

        return cleaned_stats

    def extract_episode_info_simple(self, folder_path: str) -> Dict[str, Any]:
        """ç®€å•çš„å‰§é›†ä¿¡æ¯æå–"""
        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
            return {'episodes': [], 'season_info': '', 'total_episodes': 0}

        episodes = []
        seasons = set()

        try:
            for _, _, files in os.walk(folder_path):
                for file in files:
                    if self.is_video_file(file):
                        episode_info = self.parse_episode_from_filename(file)
                        if episode_info:
                            episodes.append(episode_info)
                            if episode_info['season']:
                                seasons.add(episode_info['season'])
        except (PermissionError, OSError):
            return {'episodes': [], 'season_info': 'æ— æ³•è®¿é—®', 'total_episodes': 0}

        episodes.sort(key=lambda x: (x['season'] or 0, x['episode'] or 0))
        season_info = self.generate_season_summary(episodes, seasons)

        return {
            'episodes': episodes,
            'season_info': season_info,
            'total_episodes': len(episodes)
        }

    def parse_episode_from_filename(self, filename: str) -> Optional[Dict[str, Any]]:
        """ä»æ–‡ä»¶åä¸­è§£æå‰§é›†ä¿¡æ¯"""
        import re

        patterns = [
            (r'[Ss](\d{1,2})[Ee](\d{1,3})', 'season_episode'),
            (r'[Ss]eason\s*(\d{1,2})\s*[Ee]pisode\s*(\d{1,3})', 'season_episode'),
            (r'ç¬¬(\d{1,2})å­£ç¬¬(\d{1,3})é›†', 'season_episode'),
            (r'(\d{1,2})x(\d{1,3})', 'season_episode'),
            (r'(?:[Ee][Pp]\.?\s*(\d{1,3})|ç¬¬(\d{1,3})é›†)', 'episode_only'),
        ]

        for pattern, pattern_type in patterns:
            match = re.search(pattern, filename)
            if match:
                try:
                    if pattern_type == 'season_episode':
                        season = int(match.group(1))
                        episode = int(match.group(2))
                        if 1 <= season <= 50 and 1 <= episode <= 500:
                            return {
                                'season': season,
                                'episode': episode,
                                'filename': filename,
                                'pattern_type': pattern_type
                            }
                    elif pattern_type == 'episode_only':
                        episode = int(match.group(1) or match.group(2))
                        if 1 <= episode <= 500:
                            return {
                                'season': None,
                                'episode': episode,
                                'filename': filename,
                                'pattern_type': pattern_type
                            }
                except ValueError:
                    continue

        return None

    def generate_season_summary(self, episodes: list, seasons: set) -> str:
        """ç”Ÿæˆå­£åº¦æ‘˜è¦ä¿¡æ¯"""
        if not episodes:
            return "æ— å‰§é›†ä¿¡æ¯"

        if not seasons or None in seasons:
            episode_numbers = [ep['episode'] for ep in episodes if ep.get('episode')]
            if episode_numbers:
                return self._format_episode_range(episode_numbers)
            else:
                return f"{len(episodes)}ä¸ªè§†é¢‘"

        season_summaries = []
        for season in sorted(seasons):
            season_episodes = [ep for ep in episodes if ep.get('season') == season]
            episode_numbers = [ep['episode'] for ep in season_episodes if ep.get('episode')]

            if episode_numbers:
                episode_range = self._format_episode_range(episode_numbers)
                season_summary = f"S{season:02d}{episode_range}"
                season_summaries.append(season_summary)

        return ', '.join(season_summaries) if season_summaries else f"{len(episodes)}ä¸ªè§†é¢‘"

    def _format_episode_range(self, episode_numbers: List[int]) -> str:
        """æ ¼å¼åŒ–é›†æ•°èŒƒå›´"""
        if not episode_numbers:
            return ""

        episode_numbers = sorted(set(episode_numbers))

        if len(episode_numbers) == 1:
            return f"E{episode_numbers[0]:02d}"

        is_fully_continuous = all(
            episode_numbers[i] == episode_numbers[i-1] + 1
            for i in range(1, len(episode_numbers))
        )

        if is_fully_continuous:
            return f"E{episode_numbers[0]:02d}-E{episode_numbers[-1]:02d}"
        else:
            groups = []
            start = episode_numbers[0]
            end = episode_numbers[0]

            for i in range(1, len(episode_numbers)):
                if episode_numbers[i] == end + 1:
                    end = episode_numbers[i]
                else:
                    if start == end:
                        groups.append(f"E{start:02d}")
                    else:
                        groups.append(f"E{start:02d}-E{end:02d}")
                    start = episode_numbers[i]
                    end = episode_numbers[i]

            if start == end:
                groups.append(f"E{start:02d}")
            else:
                groups.append(f"E{start:02d}-E{end:02d}")

            return ",".join(groups)


# ================== ç§å­åˆ›å»ºå™¨ ==================
class TorrentCreator:
    """ç§å­åˆ›å»ºå™¨ - v1.5.0é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""

    DEFAULT_PIECE_SIZE = "auto"
    DEFAULT_COMMENT = "Created by Torrent Maker v1.5.0"
    PIECE_SIZES = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]

    # Piece Size æŸ¥æ‰¾è¡¨ - é¢„è®¡ç®—å¸¸ç”¨å¤§å°èŒƒå›´
    PIECE_SIZE_LOOKUP = {
        # æ–‡ä»¶å¤§å°èŒƒå›´ (MB) -> (piece_size_kb, log2_value)
        (0, 50): (16, 14),           # å°æ–‡ä»¶: 16KB pieces
        (50, 200): (32, 15),         # ä¸­å°æ–‡ä»¶: 32KB pieces
        (200, 500): (64, 16),        # ä¸­ç­‰æ–‡ä»¶: 64KB pieces
        (500, 1000): (128, 17),      # è¾ƒå¤§æ–‡ä»¶: 128KB pieces
        (1000, 2000): (256, 18),     # å¤§æ–‡ä»¶: 256KB pieces
        (2000, 5000): (512, 19),     # å¾ˆå¤§æ–‡ä»¶: 512KB pieces
        (5000, 10000): (1024, 20),   # è¶…å¤§æ–‡ä»¶: 1MB pieces
        (10000, 20000): (2048, 21),  # å·¨å¤§æ–‡ä»¶: 2MB pieces
        (20000, float('inf')): (4096, 22)  # æå¤§æ–‡ä»¶: 4MB pieces
    }

    def __init__(self, tracker_links: List[str], output_dir: str = "output",
                 piece_size: Union[str, int] = "auto", private: bool = False,
                 comment: str = None, max_workers: int = 4):
        self.tracker_links = list(tracker_links) if tracker_links else []
        self.output_dir = Path(output_dir)
        self.piece_size = piece_size
        self.private = private
        self.comment = comment or self.DEFAULT_COMMENT
        self.max_workers = max_workers

        # æ€§èƒ½ç›‘æ§å’Œç¼“å­˜
        self.performance_monitor = PerformanceMonitor()
        self.size_cache = DirectorySizeCache()
        self._piece_size_cache = {}  # ç¼“å­˜è®¡ç®—ç»“æœ

        # v1.5.0 ç¬¬äºŒé˜¶æ®µä¼˜åŒ–ï¼šå†…å­˜ç®¡ç†å’Œå¼‚æ­¥ I/O
        self.memory_manager = MemoryManager(max_memory_mb=512)
        self.stream_processor = StreamFileProcessor(memory_manager=self.memory_manager)
        self.async_processor = AsyncIOProcessor(max_concurrent=4)
        self.async_file_processor = AsyncFileProcessor(max_concurrent=6)

        if not self._check_mktorrent():
            raise TorrentCreationError("ç³»ç»Ÿæœªå®‰è£…mktorrentå·¥å…·")

    def _check_mktorrent(self) -> bool:
        return shutil.which('mktorrent') is not None

    def _ensure_output_dir(self) -> None:
        try:
            self.output_dir.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            raise TorrentCreationError(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {e}")

    def _calculate_piece_size(self, total_size: int) -> int:
        """æ™ºèƒ½è®¡ç®—åˆé€‚çš„pieceå¤§å° - é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        # æ£€æŸ¥ç¼“å­˜
        size_mb = total_size // (1024 * 1024)
        cache_key = f"size_{size_mb}"

        if cache_key in self._piece_size_cache:
            return self._piece_size_cache[cache_key]

        # ä½¿ç”¨æŸ¥æ‰¾è¡¨å¿«é€Ÿç¡®å®š piece size
        for (min_size, max_size), (_, log2_value) in self.PIECE_SIZE_LOOKUP.items():
            if min_size <= size_mb < max_size:
                self._piece_size_cache[cache_key] = log2_value
                return log2_value

        # å›é€€åˆ°ä¼ ç»Ÿè®¡ç®—æ–¹æ³•ï¼ˆç”¨äºæç«¯æƒ…å†µï¼‰
        target_pieces = 1500
        optimal_piece_size = total_size // (target_pieces * 1024)

        for size in self.PIECE_SIZES:
            if size >= optimal_piece_size:
                import math
                log2_value = int(math.log2(size * 1024))
                self._piece_size_cache[cache_key] = log2_value
                return log2_value

        # è¿”å›æœ€å¤§pieceå¤§å°çš„æŒ‡æ•°å€¼
        import math
        log2_value = int(math.log2(self.PIECE_SIZES[-1] * 1024))
        self._piece_size_cache[cache_key] = log2_value
        return log2_value

    def _get_optimal_piece_size_fast(self, total_size: int) -> Tuple[int, int]:
        """å¿«é€Ÿè·å–æœ€ä¼˜ piece sizeï¼ˆKB å’Œ log2 å€¼ï¼‰"""
        size_mb = total_size // (1024 * 1024)

        # ç›´æ¥æŸ¥è¡¨ï¼ŒO(1) æ—¶é—´å¤æ‚åº¦
        for (min_size, max_size), (piece_size_kb, log2_value) in self.PIECE_SIZE_LOOKUP.items():
            if min_size <= size_mb < max_size:
                return piece_size_kb, log2_value

        # é»˜è®¤è¿”å›æœ€å¤§å€¼
        return 4096, 22

    def _get_directory_size(self, path: Path) -> int:
        """è·å–ç›®å½•å¤§å° - ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–"""
        self.performance_monitor.start_timer('directory_size_calculation')
        try:
            size = self.size_cache.get_directory_size(path)
            return size
        finally:
            duration = self.performance_monitor.end_timer('directory_size_calculation')
            if duration > 5.0:  # å¦‚æœè®¡ç®—æ—¶é—´è¶…è¿‡5ç§’ï¼Œè®°å½•è­¦å‘Š
                logger.warning(f"ç›®å½•å¤§å°è®¡ç®—è€—æ—¶è¾ƒé•¿: {duration:.2f}s for {path}")

    def _sanitize_filename(self, filename: str) -> str:
        import re
        unsafe_chars = r'[<>:"/\\|?*]'
        sanitized = re.sub(unsafe_chars, '_', filename)
        sanitized = sanitized.strip(' .')
        return sanitized if sanitized else "torrent"

    def _build_command(self, source_path: Path, output_file: Path,
                      piece_size: int = None) -> List[str]:
        """æ„å»ºä¼˜åŒ–çš„ mktorrent å‘½ä»¤"""
        command = ['mktorrent']

        # æ·»åŠ  tracker é“¾æ¥
        for tracker in self.tracker_links:
            command.extend(['-a', tracker])

        # è®¾ç½®è¾“å‡ºæ–‡ä»¶
        command.extend(['-o', str(output_file)])

        # è®¾ç½®æ³¨é‡Šï¼ˆç®€åŒ–ä»¥å‡å°‘å¼€é”€ï¼‰
        comment = f"{self.comment} v1.5.0"
        command.extend(['-c', comment])

        # è®¾ç½® piece å¤§å°
        if piece_size:
            command.extend(['-l', str(piece_size)])

        # å¯ç”¨å¤šçº¿ç¨‹å¤„ç†ï¼ˆæ€§èƒ½ä¼˜åŒ–å…³é”®ï¼‰
        import os
        cpu_count = os.cpu_count() or 4
        # ä½¿ç”¨ CPU æ ¸å¿ƒæ•°ï¼Œä½†é™åˆ¶æœ€å¤§å€¼é¿å…è¿‡åº¦ç«äº‰
        thread_count = min(cpu_count, 8)
        command.extend(['-t', str(thread_count)])

        # ç§æœ‰ç§å­æ ‡è®°
        if self.private:
            command.append('-p')

        # å‡å°‘è¾“å‡ºä¿¡æ¯ä»¥æé«˜æ€§èƒ½ï¼ˆç§»é™¤ -v å‚æ•°ï¼‰
        # command.append('-v')  # æ³¨é‡Šæ‰è¯¦ç»†è¾“å‡º

        # æ·»åŠ æºè·¯å¾„
        command.append(str(source_path))

        return command

    def _get_mktorrent_version(self) -> str:
        """è·å– mktorrent ç‰ˆæœ¬ä¿¡æ¯"""
        try:
            result = subprocess.run(['mktorrent', '--help'],
                                  capture_output=True, text=True, timeout=5)
            if result.stdout:
                # ä»å¸®åŠ©ä¿¡æ¯ä¸­æå–ç‰ˆæœ¬
                lines = result.stdout.split('\n')
                for line in lines:
                    if 'mktorrent' in line and '(' in line:
                        return line.strip()
            return "mktorrent (version unknown)"
        except Exception:
            return "mktorrent (version unknown)"

    def create_torrent(self, source_path: Union[str, Path],
                      custom_name: str = None,
                      progress_callback = None) -> Optional[str]:
        """åˆ›å»ºç§å­æ–‡ä»¶ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        self.performance_monitor.start_timer('total_torrent_creation')

        try:
            source_path = Path(source_path)

            if not source_path.exists():
                raise TorrentCreationError(f"æºè·¯å¾„ä¸å­˜åœ¨: {source_path}")

            self._ensure_output_dir()

            if custom_name:
                torrent_name = self._sanitize_filename(custom_name)
            else:
                torrent_name = self._sanitize_filename(source_path.name)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.output_dir / f"{torrent_name}_{timestamp}.torrent"

            # æ™ºèƒ½è®¡ç®—pieceå¤§å°ï¼ˆé«˜æ€§èƒ½ä¼˜åŒ–ï¼‰
            piece_size = None
            if self.piece_size == "auto":
                self.performance_monitor.start_timer('piece_size_calculation')
                try:
                    if source_path.is_dir():
                        total_size = self._get_directory_size(source_path)
                    else:
                        total_size = source_path.stat().st_size

                    # ä½¿ç”¨ä¼˜åŒ–çš„å¿«é€Ÿè®¡ç®—æ–¹æ³•
                    piece_size = self._calculate_piece_size(total_size)

                    # è®°å½•ä¼˜åŒ–ä¿¡æ¯
                    piece_size_kb, _ = self._get_optimal_piece_size_fast(total_size)
                    print(f"  ğŸ¯ æ™ºèƒ½é€‰æ‹© Piece å¤§å°: {piece_size_kb}KB (æ–‡ä»¶å¤§å°: {total_size // (1024*1024)}MB)")

                finally:
                    self.performance_monitor.end_timer('piece_size_calculation')
            elif isinstance(self.piece_size, int):
                # å¦‚æœç”¨æˆ·è®¾ç½®çš„æ˜¯KBå€¼ï¼Œéœ€è¦è½¬æ¢ä¸ºæŒ‡æ•°å€¼
                import math
                piece_size = int(math.log2(self.piece_size * 1024))

            command = self._build_command(source_path, output_file, piece_size)

            # è®°å½•è°ƒè¯•ä¿¡æ¯
            if piece_size:
                actual_piece_size = 2 ** piece_size
                print(f"  ğŸ”§ Pieceå¤§å°: 2^{piece_size} = {actual_piece_size} bytes ({actual_piece_size // 1024} KB)")

            if progress_callback:
                progress_callback(f"æ­£åœ¨åˆ›å»ºç§å­æ–‡ä»¶: {torrent_name}")

            # æ‰§è¡Œmktorrentå‘½ä»¤ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰
            self.performance_monitor.start_timer('mktorrent_execution')
            try:
                # ä¼˜åŒ–çš„ subprocess è°ƒç”¨
                result = subprocess.run(
                    command,
                    capture_output=True,
                    text=True,
                    check=True,
                    timeout=3600,
                    # ä¼˜åŒ–ç¯å¢ƒå˜é‡ï¼Œå‡å°‘ä¸å¿…è¦çš„å¼€é”€
                    env=dict(os.environ, LANG='C', LC_ALL='C')
                )

                # è®°å½•æ‰§è¡Œç»“æœï¼ˆå¦‚æœéœ€è¦è°ƒè¯•ï¼‰
                if result.stderr:
                    logger.warning(f"mktorrent stderr: {result.stderr}")

            finally:
                mktorrent_duration = self.performance_monitor.end_timer('mktorrent_execution')
                print(f"  â±ï¸ mktorrentæ‰§è¡Œæ—¶é—´: {mktorrent_duration:.2f}s")

            if not output_file.exists():
                raise TorrentCreationError("ç§å­æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼šè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")

            # éªŒè¯ç§å­æ–‡ä»¶
            if not self.validate_torrent(output_file):
                raise TorrentCreationError("ç§å­æ–‡ä»¶éªŒè¯å¤±è´¥")

            if progress_callback:
                progress_callback(f"ç§å­æ–‡ä»¶åˆ›å»ºæˆåŠŸ: {output_file.name}")

            return str(output_file)

        except subprocess.CalledProcessError as e:
            error_msg = f"mktorrentæ‰§è¡Œå¤±è´¥: {e}"
            if e.stderr:
                error_msg += f"\né”™è¯¯ä¿¡æ¯: {e.stderr}"
            raise TorrentCreationError(error_msg)

        except subprocess.TimeoutExpired:
            raise TorrentCreationError("ç§å­åˆ›å»ºè¶…æ—¶")

        except Exception as e:
            raise TorrentCreationError(f"åˆ›å»ºç§å­æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

        finally:
            total_duration = self.performance_monitor.end_timer('total_torrent_creation')
            print(f"  ğŸ“Š æ€»è€—æ—¶: {total_duration:.2f}s")

    def create_torrents_batch(self, source_paths: List[Union[str, Path]],
                             progress_callback = None) -> List[Tuple[str, Optional[str], Optional[str]]]:
        """æ‰¹é‡åˆ›å»ºç§å­æ–‡ä»¶ - é«˜æ€§èƒ½å¹¶å‘å¤„ç†"""
        if not source_paths:
            return []

        results = []
        total_count = len(source_paths)

        # æ ¹æ®ä»»åŠ¡æ•°é‡é€‰æ‹©æœ€ä¼˜å¹¶å‘ç­–ç•¥
        if total_count <= 2:
            # å°‘é‡ä»»åŠ¡ä½¿ç”¨ä¸²è¡Œå¤„ç†ï¼Œé¿å…å¹¶å‘å¼€é”€
            for i, source_path in enumerate(source_paths):
                try:
                    if progress_callback:
                        progress_callback(f"æ­£åœ¨å¤„ç† ({i + 1}/{total_count}): {Path(source_path).name}")
                    result_path = self.create_torrent(source_path)
                    results.append((str(source_path), result_path, None))
                except Exception as e:
                    results.append((str(source_path), None, str(e)))
            return results

        def create_single_with_error_handling(args):
            index, source_path = args
            try:
                if progress_callback:
                    progress_callback(f"æ­£åœ¨å¤„ç† ({index + 1}/{total_count}): {Path(source_path).name}")

                result_path = self.create_torrent(source_path)
                return (str(source_path), result_path, None)
            except Exception as e:
                return (str(source_path), None, str(e))

        # å¯¹äº CPU å¯†é›†å‹ä»»åŠ¡ï¼Œä¼˜å…ˆä½¿ç”¨è¿›ç¨‹æ± 
        use_process_pool = total_count > 4 and self.max_workers > 2

        if use_process_pool:
            # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†å¤§æ‰¹é‡ä»»åŠ¡
            try:
                with ProcessPoolExecutor(max_workers=min(self.max_workers, total_count, 4)) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    future_to_path = {
                        executor.submit(create_single_with_error_handling, (i, path)): path
                        for i, path in enumerate(source_paths)
                    }

                    # æ”¶é›†ç»“æœ
                    for future in as_completed(future_to_path):
                        try:
                            result = future.result()
                            results.append(result)
                        except Exception as e:
                            source_path = future_to_path[future]
                            results.append((str(source_path), None, str(e)))
            except Exception as e:
                # è¿›ç¨‹æ± å¤±è´¥æ—¶å›é€€åˆ°çº¿ç¨‹æ± 
                logger.warning(f"è¿›ç¨‹æ± æ‰§è¡Œå¤±è´¥ï¼Œå›é€€åˆ°çº¿ç¨‹æ± : {e}")
                use_process_pool = False

        if not use_process_pool:
            # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†ä¸­ç­‰æ‰¹é‡ä»»åŠ¡
            with ThreadPoolExecutor(max_workers=min(self.max_workers, total_count)) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_path = {
                    executor.submit(create_single_with_error_handling, (i, path)): path
                    for i, path in enumerate(source_paths)
                }

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_path):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        source_path = future_to_path[future]
                        results.append((str(source_path), None, str(e)))

        return results

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ - v1.5.0 ç¬¬äºŒé˜¶æ®µå¢å¼ºç‰ˆ"""
        stats = self.performance_monitor.get_all_stats()
        cache_stats = self.size_cache.get_cache_stats()
        memory_info = self.memory_manager.get_memory_usage()

        # è®¡ç®—æ€§èƒ½æ”¹è¿›æŒ‡æ ‡
        creation_stats = stats.get('total_torrent_creation', {})
        mktorrent_stats = stats.get('mktorrent_execution', {})
        piece_calc_stats = stats.get('piece_size_calculation', {})

        return {
            'performance': stats,
            'cache': cache_stats,
            'memory_management': {
                'current_usage_mb': memory_info.get('rss_mb', 0),
                'memory_limit_mb': self.memory_manager.max_memory_mb,
                'memory_efficiency': self._calculate_memory_efficiency(memory_info),
                'cleanup_needed': self.memory_manager.should_cleanup()
            },
            'piece_size_cache': {
                'cached_calculations': len(self._piece_size_cache),
                'cache_entries': list(self._piece_size_cache.keys())[:5]  # æ˜¾ç¤ºå‰5ä¸ª
            },
            'async_processing': {
                'max_concurrent_operations': self.async_processor.max_concurrent,
                'stream_chunk_size_mb': self.stream_processor.base_chunk_size / (1024 * 1024)
            },
            'summary': {
                'total_torrents_created': creation_stats.get('count', 0),
                'average_creation_time': creation_stats.get('average', 0),
                'average_mktorrent_time': mktorrent_stats.get('average', 0),
                'average_size_calculation_time': stats.get('directory_size_calculation', {}).get('average', 0),
                'average_piece_calculation_time': piece_calc_stats.get('average', 0),
                'cache_hit_rate': cache_stats.get('hit_rate', 0),
                'memory_usage_mb': memory_info.get('rss_mb', 0),
                'performance_grade': self._calculate_performance_grade_v2(creation_stats, cache_stats, memory_info)
            },
            'optimization_suggestions': self._generate_optimization_suggestions_v2(stats, cache_stats, memory_info)
        }

    def _calculate_memory_efficiency(self, memory_info: Dict) -> str:
        """è®¡ç®—å†…å­˜ä½¿ç”¨æ•ˆç‡"""
        usage_mb = memory_info.get('rss_mb', 0)
        limit_mb = self.memory_manager.max_memory_mb

        if usage_mb == 0:
            return "æœªçŸ¥"

        efficiency = (limit_mb - usage_mb) / limit_mb
        if efficiency > 0.7:
            return "ä¼˜ç§€"
        elif efficiency > 0.5:
            return "è‰¯å¥½"
        elif efficiency > 0.3:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦ä¼˜åŒ–"

    def _calculate_performance_grade_v2(self, creation_stats: Dict, cache_stats: Dict, memory_info: Dict) -> str:
        """è®¡ç®—æ€§èƒ½ç­‰çº§ - v1.5.0 ç¬¬äºŒé˜¶æ®µç‰ˆæœ¬"""
        avg_time = creation_stats.get('average', 0)
        hit_rate = cache_stats.get('hit_rate', 0)
        memory_mb = memory_info.get('rss_mb', 0)

        # ç»¼åˆè¯„åˆ†ç³»ç»Ÿ
        time_score = 100 if avg_time < 10 else max(0, 100 - (avg_time - 10) * 3)
        cache_score = hit_rate * 100
        memory_score = max(0, 100 - memory_mb / 5)  # 500MB ä¸ºæ»¡åˆ†

        total_score = (time_score * 0.4 + cache_score * 0.3 + memory_score * 0.3)

        if total_score >= 90:
            return "ä¼˜ç§€ (A+)"
        elif total_score >= 80:
            return "è‰¯å¥½ (B+)"
        elif total_score >= 70:
            return "ä¸€èˆ¬ (C+)"
        elif total_score >= 60:
            return "åŠæ ¼ (D+)"
        else:
            return "éœ€è¦ä¼˜åŒ– (F)"

    def _generate_optimization_suggestions_v2(self, stats: Dict, cache_stats: Dict, memory_info: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®® - v1.5.0 ç¬¬äºŒé˜¶æ®µç‰ˆæœ¬"""
        suggestions = []

        # æ£€æŸ¥åˆ›å»ºæ—¶é—´
        creation_avg = stats.get('total_torrent_creation', {}).get('average', 0)
        if creation_avg > 30:
            suggestions.append("ç§å­åˆ›å»ºæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®æ£€æŸ¥ç£ç›˜æ€§èƒ½æˆ–å‡å°‘æ–‡ä»¶æ•°é‡")
        elif creation_avg > 15:
            suggestions.append("ç§å­åˆ›å»ºæ—¶é—´åé•¿ï¼Œå¯ä»¥è€ƒè™‘è°ƒæ•´ piece size æˆ–å¯ç”¨æ›´å¤šå¹¶å‘")

        # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
        hit_rate = cache_stats.get('hit_rate', 0)
        if hit_rate < 0.3:
            suggestions.append("ç¼“å­˜å‘½ä¸­ç‡å¾ˆä½ï¼Œå»ºè®®å¢åŠ ç¼“å­˜æ—¶é—´æˆ–æ£€æŸ¥é‡å¤æ“ä½œæ¨¡å¼")
        elif hit_rate < 0.6:
            suggestions.append("ç¼“å­˜å‘½ä¸­ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory_mb = memory_info.get('rss_mb', 0)
        if memory_mb > 400:
            suggestions.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®å¯ç”¨å†…å­˜æ¸…ç†æˆ–å‡å°‘ç¼“å­˜å¤§å°")
        elif memory_mb > 300:
            suggestions.append("å†…å­˜ä½¿ç”¨åé«˜ï¼Œå»ºè®®ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")

        # æ£€æŸ¥ mktorrent æ‰§è¡Œæ—¶é—´
        mktorrent_avg = stats.get('mktorrent_execution', {}).get('average', 0)
        if mktorrent_avg > 20:
            suggestions.append("mktorrent æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®æ£€æŸ¥ CPU æ€§èƒ½æˆ–è°ƒæ•´ piece size")

        # æ£€æŸ¥ç›®å½•æ‰«ææ€§èƒ½
        scan_avg = stats.get('directory_size_calculation', {}).get('average', 0)
        if scan_avg > 5:
            suggestions.append("ç›®å½•æ‰«æè¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨ SSD æˆ–å‡å°‘æ‰«ææ·±åº¦")

        if not suggestions:
            suggestions.append("ğŸ‰ æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼æ‰€æœ‰æŒ‡æ ‡éƒ½åœ¨æœ€ä½³èŒƒå›´å†…")

        return suggestions

    def _calculate_performance_grade(self, creation_stats: Dict, cache_stats: Dict) -> str:
        """è®¡ç®—æ€§èƒ½ç­‰çº§ - å…¼å®¹æ€§æ–¹æ³•"""
        return self._calculate_performance_grade_v2(creation_stats, cache_stats, {'rss_mb': 0})

    def _generate_optimization_suggestions(self, stats: Dict, cache_stats: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®® - å…¼å®¹æ€§æ–¹æ³•"""
        return self._generate_optimization_suggestions_v2(stats, cache_stats, {'rss_mb': 0})

    def clear_caches(self) -> Dict[str, int]:
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜ - v1.5.0 ç¬¬äºŒé˜¶æ®µå¢å¼ºç‰ˆ"""
        cleared_counts = {}

        # æ¸…ç†ç›®å½•å¤§å°ç¼“å­˜
        self.size_cache.clear_cache()
        cleared_counts['directory_size_cache'] = 0

        # æ¸…ç† piece size ç¼“å­˜
        piece_cache_count = len(self._piece_size_cache)
        self._piece_size_cache.clear()
        cleared_counts['piece_size_cache'] = piece_cache_count

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        expired_count = self.size_cache.cleanup_expired()
        cleared_counts['expired_entries'] = expired_count

        # v1.5.0 æ–°å¢ï¼šæ·±åº¦å†…å­˜ç®¡ç†æ¸…ç†
        memory_cleaned = self.memory_manager.cleanup_memory()
        cleared_counts.update(memory_cleaned)

        # è·å–å†…å­˜åˆ†æ
        memory_analysis = self.memory_manager.get_memory_analysis()
        cleared_counts['memory_analysis'] = {
            'freed_mb': memory_cleaned.get('freed_mb', 0),
            'trend': memory_analysis['memory_trend']['trend'],
            'recommendations': memory_analysis['recommendations'][:2]  # åªæ˜¾ç¤ºå‰2ä¸ªå»ºè®®
        }

        return cleared_counts

    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯ - v1.5.0 æ–°å¢"""
        memory_info = self.memory_manager.get_memory_usage()

        return {
            'version': '1.5.0',
            'optimization_level': 'Stage 2 - Advanced',
            'features': [
                'Smart Piece Size Calculation',
                'LRU Directory Cache',
                'Multi-threaded mktorrent',
                'Intelligent Search Index',
                'Memory Management',
                'Async I/O Processing',
                'Stream File Processing'
            ],
            'memory_info': memory_info,
            'performance_grade': self._calculate_performance_grade_v2({}, {}, memory_info),
            'cache_status': {
                'directory_cache_size': len(self.size_cache._cache) if hasattr(self.size_cache, '_cache') else 0,
                'piece_cache_size': len(self._piece_size_cache)
            }
        }

    def validate_torrent(self, torrent_path: Union[str, Path]) -> bool:
        """éªŒè¯ç§å­æ–‡ä»¶çš„æœ‰æ•ˆæ€§"""
        try:
            torrent_path = Path(torrent_path)

            if not torrent_path.exists():
                return False

            if not torrent_path.suffix.lower() == '.torrent':
                return False

            file_size = torrent_path.stat().st_size
            if file_size == 0:
                return False

            try:
                with open(torrent_path, 'rb') as f:
                    header = f.read(10)
                    if not header.startswith(b'd'):
                        return False
            except Exception:
                return False

            return True

        except Exception:
            return False


# ================== æœç´¢å†å²ç®¡ç† ==================
class SearchHistory:
    """æœç´¢å†å²ç®¡ç†å™¨"""

    def __init__(self, config_dir: str = None, max_history: int = 50):
        """åˆå§‹åŒ–æœç´¢å†å²ç®¡ç†å™¨"""
        if config_dir is None:
            config_dir = os.path.expanduser("~/.torrent_maker")

        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(parents=True, exist_ok=True)

        self.history_file = self.config_dir / "search_history.json"
        self.max_history = max_history
        self.history: List[Dict[str, Any]] = []

        self._load_history()

    def _load_history(self):
        """åŠ è½½æœç´¢å†å²"""
        try:
            if self.history_file.exists():
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.history = data.get('history', [])
                    self._cleanup_old_history()
            else:
                self.history = []
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æœç´¢å†å²å¤±è´¥: {e}")
            self.history = []

    def _save_history(self):
        """ä¿å­˜æœç´¢å†å²"""
        try:
            data = {
                'history': self.history,
                'last_updated': datetime.now().isoformat()
            }
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æœç´¢å†å²å¤±è´¥: {e}")

    def _cleanup_old_history(self):
        """æ¸…ç†è¿‡æœŸçš„å†å²è®°å½•"""
        try:
            from datetime import timedelta
            cutoff_time = datetime.now() - timedelta(days=30)

            self.history = [
                item for item in self.history
                if datetime.fromisoformat(item.get('timestamp', '1970-01-01'))
                > cutoff_time
            ]

            if len(self.history) > self.max_history:
                self.history = self.history[-self.max_history:]

        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å†å²è®°å½•å¤±è´¥: {e}")

    def add_search(self, query: str, results_count: int = 0,
                   resource_folder: str = None) -> None:
        """æ·»åŠ æœç´¢è®°å½•"""
        if not query or not query.strip():
            return

        query = query.strip()

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æœç´¢
        recent_queries = [item['query'] for item in self.history[-10:]]
        if query in recent_queries:
            for item in reversed(self.history):
                if item['query'] == query:
                    item['timestamp'] = datetime.now().isoformat()
                    item['count'] = item.get('count', 0) + 1
                    item['last_results_count'] = results_count
                    if resource_folder:
                        item['resource_folder'] = resource_folder
                    break
        else:
            record = {
                'query': query,
                'timestamp': datetime.now().isoformat(),
                'results_count': results_count,
                'count': 1,
                'last_results_count': results_count
            }

            if resource_folder:
                record['resource_folder'] = resource_folder

            self.history.append(record)

        if len(self.history) > self.max_history:
            self.history = self.history[-self.max_history:]

        self._save_history()

    def get_recent_searches(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„æœç´¢è®°å½•"""
        sorted_history = sorted(
            self.history,
            key=lambda x: x.get('timestamp', ''),
            reverse=True
        )
        return sorted_history[:limit]

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æœç´¢å†å²ç»Ÿè®¡ä¿¡æ¯"""
        if not self.history:
            return {
                'total_searches': 0,
                'unique_queries': 0,
                'average_results': 0,
                'most_searched': None,
                'recent_activity': 0
            }

        total_searches = sum(item.get('count', 1) for item in self.history)
        unique_queries = len(self.history)

        results_counts = [item.get('last_results_count', 0) for item in self.history]
        average_results = sum(results_counts) / len(results_counts) if results_counts else 0

        most_searched = max(self.history, key=lambda x: x.get('count', 0))

        from datetime import timedelta
        recent_cutoff = datetime.now() - timedelta(days=7)
        recent_activity = sum(
            1 for item in self.history
            if datetime.fromisoformat(item.get('timestamp', '1970-01-01')) > recent_cutoff
        )

        return {
            'total_searches': total_searches,
            'unique_queries': unique_queries,
            'average_results': round(average_results, 1),
            'most_searched': most_searched,
            'recent_activity': recent_activity
        }

    def clear_history(self) -> bool:
        """æ¸…ç©ºæœç´¢å†å²"""
        try:
            self.history = []
            self._save_history()
            return True
        except Exception as e:
            print(f"âŒ æ¸…ç©ºæœç´¢å†å²å¤±è´¥: {e}")
            return False


# ================== ä¸»ç¨‹åº ==================
class TorrentMakerApp:
    """Torrent Maker ä¸»åº”ç”¨ç¨‹åº - v1.4.0"""

    def __init__(self):
        self.config = ConfigManager()
        self.matcher = None
        self.creator = None
        self.search_history = SearchHistory()
        self._init_components()

    def _init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            # åˆå§‹åŒ–æ–‡ä»¶åŒ¹é…å™¨
            resource_folder = self.config.get_resource_folder()
            enable_cache = self.config.settings.get('enable_cache', True)
            cache_duration = self.config.settings.get('cache_duration', 3600)
            max_workers = self.config.settings.get('max_concurrent_operations', 4)

            self.matcher = FileMatcher(
                resource_folder,
                enable_cache=enable_cache,
                cache_duration=cache_duration,
                max_workers=max_workers
            )

            # åˆå§‹åŒ–ç§å­åˆ›å»ºå™¨
            trackers = self.config.get_trackers()
            output_folder = self.config.get_output_folder()

            self.creator = TorrentCreator(
                trackers,
                output_folder,
                max_workers=max_workers
            )

        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)

    def display_header(self):
        """æ˜¾ç¤ºç¨‹åºå¤´éƒ¨ä¿¡æ¯"""
        print("ğŸ¬" + "=" * 60)
        print("           Torrent Maker v1.5.0 - é«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆ")
        print("           åŸºäº mktorrent çš„åŠè‡ªåŠ¨åŒ–ç§å­åˆ¶ä½œå·¥å…·")
        print("=" * 62)
        print()
        print("ï¿½ v1.5.0 æ€§èƒ½ä¼˜åŒ–æ›´æ–°:")
        print("  âš¡ ç§å­åˆ›å»ºé€Ÿåº¦æå‡ 30-50%")
        print("  ğŸ§  æ™ºèƒ½ Piece Size è®¡ç®—ä¼˜åŒ–")
        print("  ï¿½ ç›®å½•å¤§å°ç¼“å­˜ LRU ä¼˜åŒ–")
        print("  ğŸ”§ mktorrent å¤šçº¿ç¨‹å‚æ•°ä¼˜åŒ–")
        print("  ğŸš€ æ‰¹é‡å¤„ç†å¹¶å‘ä¼˜åŒ–")
        print("  ï¿½ å¢å¼ºæ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡")
        print()

    def display_menu(self):
        """æ˜¾ç¤ºä¸»èœå•"""
        print("ğŸ“‹ ä¸»èœå•:")
        print("  1. ğŸ” æœç´¢å¹¶åˆ¶ä½œç§å­")
        print("  2. âš¡ å¿«é€Ÿåˆ¶ç§ (ç›´æ¥è¾“å…¥è·¯å¾„)")
        print("  3. ğŸ“ æ‰¹é‡åˆ¶ç§")
        print("  4. âš™ï¸  é…ç½®ç®¡ç†")
        print("  5. ğŸ“Š æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡")
        print("  6. â“ å¸®åŠ©")
        print("  0. ğŸšª é€€å‡º")
        print()

    def search_and_create(self):
        """æœç´¢å¹¶åˆ¶ä½œç§å­"""
        while True:
            search_name = input("ğŸ” è¯·è¾“å…¥è¦æœç´¢çš„å½±è§†å‰§åç§° (å›è½¦è¿”å›ä¸»èœå•): ").strip()
            if not search_name:
                break

            print(f"\nğŸ”„ æ­£åœ¨æœç´¢ '{search_name}'...")
            start_time = time.time()

            try:
                results = self.matcher.match_folders(search_name)
                search_time = time.time() - start_time

                if not results:
                    print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¤¹ (æœç´¢è€—æ—¶: {search_time:.3f}s)")
                    # è¯¢é—®æ˜¯å¦ç»§ç»­æœç´¢
                    while True:
                        continue_choice = input("æ˜¯å¦ç»§ç»­æœç´¢å…¶ä»–å†…å®¹ï¼Ÿ(y/n): ").strip().lower()
                        if continue_choice in ['n', 'no', 'å¦']:
                            return  # è¿”å›ä¸»èœå•
                        elif continue_choice in ['y', 'yes', 'æ˜¯', '']:
                            break  # ç»§ç»­æœç´¢å¾ªç¯
                        else:
                            print("è¯·è¾“å…¥ y(æ˜¯) æˆ– n(å¦)")
                    continue

                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æœ (æœç´¢è€—æ—¶: {search_time:.3f}s)")
                print()

                # æ˜¾ç¤ºæœç´¢ç»“æœ
                for i, result in enumerate(results, 1):
                    status = "âœ…" if result['readable'] else "âŒ"
                    print(f"  {i:2d}. {status} {result['name']}")
                    print(f"      ğŸ“Š åŒ¹é…åº¦: {result['score']}% | ğŸ“ æ–‡ä»¶: {result['file_count']}ä¸ª | ğŸ’¾ å¤§å°: {result['size']}")
                    if result['episodes']:
                        print(f"      ğŸ¬ å‰§é›†: {result['episodes']}")
                    # æ˜¾ç¤ºæ–‡ä»¶å¤¹è·¯å¾„
                    folder_path = result['path']
                    # å¦‚æœè·¯å¾„å¤ªé•¿ï¼Œæ˜¾ç¤ºç›¸å¯¹è·¯å¾„æˆ–ç¼©çŸ­è·¯å¾„
                    if len(folder_path) > 80:
                        # å°è¯•æ˜¾ç¤ºç›¸å¯¹äºèµ„æºæ–‡ä»¶å¤¹çš„è·¯å¾„
                        resource_folder = self.config.get_resource_folder()
                        if folder_path.startswith(resource_folder):
                            relative_path = os.path.relpath(folder_path, resource_folder)
                            print(f"      ğŸ“‚ è·¯å¾„: .../{relative_path}")
                        else:
                            # å¦‚æœè·¯å¾„å¤ªé•¿ï¼Œæ˜¾ç¤ºå¼€å¤´å’Œç»“å°¾
                            print(f"      ğŸ“‚ è·¯å¾„: {folder_path[:30]}...{folder_path[-30:]}")
                    else:
                        print(f"      ğŸ“‚ è·¯å¾„: {folder_path}")
                    print()

                # é€‰æ‹©æ–‡ä»¶å¤¹
                choice = input("è¯·é€‰æ‹©è¦åˆ¶ä½œç§å­çš„æ–‡ä»¶å¤¹ç¼–å· (æ”¯æŒå¤šé€‰ï¼Œå¦‚: 1,3,5ï¼Œå›è½¦è·³è¿‡): ").strip()
                if not choice:
                    # è¯¢é—®æ˜¯å¦ç»§ç»­æœç´¢
                    while True:
                        continue_choice = input("æ˜¯å¦ç»§ç»­æœç´¢å…¶ä»–å†…å®¹ï¼Ÿ(y/n): ").strip().lower()
                        if continue_choice in ['n', 'no', 'å¦']:
                            return  # è¿”å›ä¸»èœå•
                        elif continue_choice in ['y', 'yes', 'æ˜¯', '']:
                            break  # ç»§ç»­æœç´¢å¾ªç¯
                        else:
                            print("è¯·è¾“å…¥ y(æ˜¯) æˆ– n(å¦)")
                    continue

                # è§£æé€‰æ‹©å¹¶æ‰§è¡Œæ‰¹é‡åˆ¶ç§
                selected_results = self._parse_selection(choice, results)
                if selected_results:
                    self._execute_batch_creation(selected_results)
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©æ ¼å¼")
                    # è¯¢é—®æ˜¯å¦ç»§ç»­æœç´¢
                    while True:
                        continue_choice = input("æ˜¯å¦ç»§ç»­æœç´¢å…¶ä»–å†…å®¹ï¼Ÿ(y/n): ").strip().lower()
                        if continue_choice in ['n', 'no', 'å¦']:
                            return  # è¿”å›ä¸»èœå•
                        elif continue_choice in ['y', 'yes', 'æ˜¯', '']:
                            break  # ç»§ç»­æœç´¢å¾ªç¯
                        else:
                            print("è¯·è¾“å…¥ y(æ˜¯) æˆ– n(å¦)")
                    continue

                # è¯¢é—®æ˜¯å¦ç»§ç»­æœç´¢
                while True:
                    continue_choice = input("\næ˜¯å¦ç»§ç»­æœç´¢å…¶ä»–å†…å®¹ï¼Ÿ(y/n): ").strip().lower()
                    if continue_choice in ['n', 'no', 'å¦']:
                        return  # è¿”å›ä¸»èœå•
                    elif continue_choice in ['y', 'yes', 'æ˜¯', '']:
                        break  # ç»§ç»­æœç´¢å¾ªç¯
                    else:
                        print("è¯·è¾“å…¥ y(æ˜¯) æˆ– n(å¦)")

            except Exception as e:
                print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                # å‘ç”Ÿé”™è¯¯æ—¶ä¹Ÿè¯¢é—®æ˜¯å¦ç»§ç»­
                while True:
                    continue_choice = input("\næ˜¯å¦ç»§ç»­æœç´¢å…¶ä»–å†…å®¹ï¼Ÿ(y/n): ").strip().lower()
                    if continue_choice in ['n', 'no', 'å¦']:
                        return  # è¿”å›ä¸»èœå•
                    elif continue_choice in ['y', 'yes', 'æ˜¯', '']:
                        break  # ç»§ç»­æœç´¢å¾ªç¯
                    else:
                        print("è¯·è¾“å…¥ y(æ˜¯) æˆ– n(å¦)")

    def _create_single_torrent(self, folder_info: Dict[str, Any]) -> bool:
        """åˆ›å»ºå•ä¸ªç§å­æ–‡ä»¶"""
        try:
            folder_path = folder_info['path']
            folder_name = folder_info['name']

            print(f"\nğŸ”„ æ­£åœ¨ä¸º '{folder_name}' åˆ›å»ºç§å­...")

            def progress_callback(message):
                print(f"  ğŸ“ˆ {message}")

            torrent_path = self.creator.create_torrent(
                folder_path,
                folder_name,
                progress_callback
            )

            if torrent_path and self.creator.validate_torrent(torrent_path):
                print(f"âœ… ç§å­åˆ›å»ºæˆåŠŸ: {os.path.basename(torrent_path)}")
                return True
            else:
                print(f"âŒ ç§å­åˆ›å»ºå¤±è´¥æˆ–éªŒè¯å¤±è´¥")
                return False

        except Exception as e:
            print(f"âŒ åˆ›å»ºç§å­æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def quick_create(self):
        """å¿«é€Ÿåˆ¶ç§"""
        print("\nâš¡ å¿«é€Ÿåˆ¶ç§æ¨¡å¼")
        print("æ”¯æŒæ ¼å¼:")
        print("  - å•ä¸ªè·¯å¾„: /path/to/folder")
        print("  - å¤šä¸ªè·¯å¾„: /path1;/path2;/path3")
        print()

        paths_input = input("è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„: ").strip()
        if not paths_input:
            return

        paths = [p.strip() for p in paths_input.split(';') if p.strip()]

        success_count = 0
        for path in paths:
            expanded_path = os.path.expanduser(path)
            if os.path.exists(expanded_path):
                folder_info = {
                    'path': expanded_path,
                    'name': os.path.basename(expanded_path)
                }
                if self._create_single_torrent(folder_info):
                    success_count += 1
            else:
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {expanded_path}")

        print(f"\nğŸ‰ å¿«é€Ÿåˆ¶ç§å®Œæˆ: æˆåŠŸ {success_count}/{len(paths)}")

    def batch_create(self):
        """ç»Ÿä¸€çš„æ‰¹é‡åˆ¶ç§åŠŸèƒ½"""
        print("\nğŸ“¦ æ‰¹é‡åˆ¶ç§")
        print("=" * 50)
        print("é€‰æ‹©æ‰¹é‡åˆ¶ç§æ–¹å¼:")
        print("1. ğŸ” æœç´¢å¹¶é€‰æ‹©æ–‡ä»¶å¤¹")
        print("2. ğŸ“ ç›´æ¥è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„")
        print("0. ğŸ”™ è¿”å›ä¸»èœå•")
        print()

        choice = input("è¯·é€‰æ‹©æ–¹å¼ (0-2): ").strip()

        if choice == '0':
            return
        elif choice == '1':
            self._batch_create_from_search()
        elif choice == '2':
            self._batch_create_from_paths()
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

    def _batch_create_from_search(self):
        """ä»æœç´¢ç»“æœä¸­æ‰¹é‡åˆ¶ç§"""
        print("\nğŸ” æœç´¢æ–‡ä»¶å¤¹è¿›è¡Œæ‰¹é‡åˆ¶ç§")
        print("=" * 40)

        search_name = input("è¯·è¾“å…¥è¦æœç´¢çš„å½±è§†å‰§åç§°: ").strip()
        if not search_name:
            print("âŒ æœç´¢åç§°ä¸èƒ½ä¸ºç©º")
            return

        print(f"\nğŸ”„ æ­£åœ¨æœç´¢ '{search_name}'...")
        start_time = time.time()

        try:
            results = self.matcher.match_folders(search_name)
            search_time = time.time() - start_time

            if not results:
                print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¤¹ (æœç´¢è€—æ—¶: {search_time:.3f}s)")
                return

            print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ç»“æœ (æœç´¢è€—æ—¶: {search_time:.3f}s)")
            print()

            # æ˜¾ç¤ºæœç´¢ç»“æœ
            for i, result in enumerate(results, 1):
                status = "âœ…" if result['readable'] else "âŒ"
                print(f"  {i:2d}. {status} {result['name']}")
                print(f"      ğŸ“Š åŒ¹é…åº¦: {result['score']}% | ğŸ“ æ–‡ä»¶: {result['file_count']}ä¸ª | ğŸ’¾ å¤§å°: {result['size']}")
                if result['episodes']:
                    print(f"      ğŸ¬ å‰§é›†: {result['episodes']}")
                print(f"      ğŸ“‚ è·¯å¾„: {self._format_path_display(result['path'])}")
                print()

            # é€‰æ‹©æ–‡ä»¶å¤¹è¿›è¡Œæ‰¹é‡åˆ¶ç§
            choice = input("è¯·é€‰æ‹©è¦åˆ¶ä½œç§å­çš„æ–‡ä»¶å¤¹ç¼–å· (æ”¯æŒå¤šé€‰ï¼Œå¦‚: 1,3,5 æˆ– 1-5ï¼Œå›è½¦å–æ¶ˆ): ").strip()
            if not choice:
                print("âŒ å·²å–æ¶ˆæ‰¹é‡åˆ¶ç§")
                return

            # è§£æé€‰æ‹©
            selected_results = self._parse_selection(choice, results)
            if not selected_results:
                print("âŒ æ— æ•ˆçš„é€‰æ‹©")
                return

            # æ‰§è¡Œæ‰¹é‡åˆ¶ç§
            self._execute_batch_creation(selected_results)

        except Exception as e:
            print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    def _batch_create_from_paths(self):
        """ä»ç›´æ¥è¾“å…¥çš„è·¯å¾„æ‰¹é‡åˆ¶ç§"""
        print("\nğŸ“ ç›´æ¥è¾“å…¥è·¯å¾„è¿›è¡Œæ‰¹é‡åˆ¶ç§")
        print("=" * 40)
        print("ğŸ’¡ æç¤ºï¼šè¾“å…¥å¤šä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ª")
        print("ğŸ’¡ è¾“å…¥ç©ºè¡Œç»“æŸè¾“å…¥")
        print("ğŸ’¡ æ”¯æŒæ‹–æ‹½æ–‡ä»¶å¤¹åˆ°ç»ˆç«¯")
        print()

        paths = []
        print("è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸï¼‰:")

        while True:
            path = input(f"è·¯å¾„ {len(paths) + 1}: ").strip()
            if not path:
                break

            # æ¸…ç†è·¯å¾„
            path = path.strip('"\'')
            path = os.path.expanduser(path)

            if not os.path.exists(path):
                print(f"âš ï¸ è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡: {path}")
                continue

            if not os.path.isdir(path):
                print(f"âš ï¸ ä¸æ˜¯æ–‡ä»¶å¤¹ï¼Œè·³è¿‡: {path}")
                continue

            paths.append(path)
            print(f"âœ… å·²æ·»åŠ : {os.path.basename(path)}")

        if not paths:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è·¯å¾„")
            return

        # è½¬æ¢ä¸ºç»“æœæ ¼å¼ä»¥ä¾¿ç»Ÿä¸€å¤„ç†
        results = []
        for path in paths:
            results.append({
                'path': path,
                'name': os.path.basename(path),
                'readable': True
            })

        # æ‰§è¡Œæ‰¹é‡åˆ¶ç§
        self._execute_batch_creation(results)

    def _format_path_display(self, folder_path: str) -> str:
        """æ ¼å¼åŒ–è·¯å¾„æ˜¾ç¤º"""
        # å¦‚æœè·¯å¾„å¤ªé•¿ï¼Œæ˜¾ç¤ºç›¸å¯¹è·¯å¾„æˆ–ç¼©çŸ­è·¯å¾„
        if len(folder_path) > 80:
            # å°è¯•æ˜¾ç¤ºç›¸å¯¹äºèµ„æºæ–‡ä»¶å¤¹çš„è·¯å¾„
            resource_folder = self.config.get_resource_folder()
            if folder_path.startswith(resource_folder):
                relative_path = os.path.relpath(folder_path, resource_folder)
                return f".../{relative_path}"
            else:
                # å¦‚æœè·¯å¾„å¤ªé•¿ï¼Œæ˜¾ç¤ºå¼€å¤´å’Œç»“å°¾
                return f"{folder_path[:30]}...{folder_path[-30:]}"
        else:
            return folder_path

    def _parse_selection(self, choice: str, results: list) -> list:
        """è§£æç”¨æˆ·é€‰æ‹©çš„æ–‡ä»¶å¤¹"""
        selected_results = []
        try:
            selected_indices = []
            for part in choice.split(','):
                part = part.strip()
                if '-' in part:
                    start, end = map(int, part.split('-'))
                    selected_indices.extend(range(start, end + 1))
                else:
                    selected_indices.append(int(part))

            # éªŒè¯é€‰æ‹©å¹¶æ”¶é›†ç»“æœ
            for idx in selected_indices:
                if 1 <= idx <= len(results):
                    selected_results.append(results[idx - 1])
                else:
                    print(f"âš ï¸ å¿½ç•¥æ— æ•ˆç¼–å·: {idx}")

        except ValueError:
            print("âŒ æ— æ•ˆçš„é€‰æ‹©æ ¼å¼")
            return []

        return selected_results

    def _execute_batch_creation(self, selected_results: list):
        """æ‰§è¡Œæ‰¹é‡åˆ¶ç§"""
        if not selected_results:
            print("âŒ æ²¡æœ‰é€‰æ‹©ä»»ä½•æ–‡ä»¶å¤¹")
            return

        print(f"\nğŸ“‹ å°†è¦å¤„ç† {len(selected_results)} ä¸ªæ–‡ä»¶å¤¹:")
        for i, result in enumerate(selected_results, 1):
            print(f"  {i}. {result['name']}")

        confirm = input(f"\nç¡®è®¤æ‰¹é‡åˆ¶ç§è¿™ {len(selected_results)} ä¸ªæ–‡ä»¶å¤¹? (y/N): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("âŒ å·²å–æ¶ˆæ‰¹é‡åˆ¶ç§")
            return

        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡åˆ¶ç§...")
        print("=" * 50)

        # æ‰¹é‡åˆ›å»ºç§å­
        success_count = 0
        for i, result in enumerate(selected_results, 1):
            print(f"\n[{i}/{len(selected_results)}] æ­£åœ¨å¤„ç†: {result['name']}")
            if self._create_single_torrent(result):
                success_count += 1

        print(f"\nğŸ‰ æ‰¹é‡åˆ¶ç§å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {success_count}/{len(selected_results)}")
        if success_count < len(selected_results):
            print(f"âŒ å¤±è´¥: {len(selected_results) - success_count}")
        print(f"âœ… æˆåŠŸç‡: {success_count/len(selected_results)*100:.1f}%")

    def config_management(self):
        """é…ç½®ç®¡ç†"""
        while True:
            print("\n" + "=" * 50)
            print("           âš™ï¸ é…ç½®ç®¡ç†")
            print("=" * 50)
            print("1. ğŸ“ æŸ¥çœ‹å½“å‰é…ç½®")
            print("2. ğŸ”§ è®¾ç½®èµ„æºæ–‡ä»¶å¤¹")
            print("3. ğŸ“‚ è®¾ç½®è¾“å‡ºæ–‡ä»¶å¤¹")
            print("4. ğŸŒ ç®¡ç† Tracker")
            print("5. ğŸ”„ é‡æ–°åŠ è½½é…ç½®")
            print("6. ğŸ“¤ å¯¼å‡ºé…ç½®")
            print("7. ğŸ“¥ å¯¼å…¥é…ç½®")
            print("8. ğŸ”„ é‡ç½®ä¸ºé»˜è®¤é…ç½®")
            print("0. ğŸ”™ è¿”å›ä¸»èœå•")
            print("=" * 50)

            choice = input("è¯·é€‰æ‹©æ“ä½œ (0-8): ").strip()

            try:
                if choice == '0':
                    break
                elif choice == '1':
                    self._show_current_config()
                elif choice == '2':
                    self._set_resource_folder()
                elif choice == '3':
                    self._set_output_folder()
                elif choice == '4':
                    self._manage_trackers()
                elif choice == '5':
                    self._reload_config()
                elif choice == '6':
                    self._export_config()
                elif choice == '7':
                    self._import_config()
                elif choice == '8':
                    self._reset_config()
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 0-8 ä¹‹é—´çš„æ•°å­—")

            except Exception as e:
                print(f"âŒ æ“ä½œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print("è¯·é‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ")

            if choice != '0':
                input("\næŒ‰å›è½¦é”®ç»§ç»­...")

    def _show_current_config(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        print("\n" + "=" * 60)
        print("           ğŸ“‹ å½“å‰é…ç½®ä¿¡æ¯")
        print("=" * 60)

        # åŸºæœ¬è·¯å¾„é…ç½®
        resource_folder = self.config.get_resource_folder()
        output_folder = self.config.get_output_folder()

        print(f"ğŸ“ èµ„æºæ–‡ä»¶å¤¹: {resource_folder}")
        print(f"   {'âœ… å­˜åœ¨' if os.path.exists(resource_folder) else 'âŒ ä¸å­˜åœ¨'}")

        print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")
        print(f"   {'âœ… å­˜åœ¨' if os.path.exists(output_folder) else 'âš ï¸ å°†è‡ªåŠ¨åˆ›å»º'}")

        # Tracker é…ç½®
        trackers = self.config.get_trackers()
        print(f"ğŸŒ Tracker é…ç½®: {len(trackers)} ä¸ª")
        if trackers:
            print("   å‰3ä¸ª Tracker:")
            for i, tracker in enumerate(trackers[:3], 1):
                print(f"   {i}. {tracker}")
            if len(trackers) > 3:
                print(f"   ... è¿˜æœ‰ {len(trackers) - 3} ä¸ª")
        else:
            print("   âŒ æœªé…ç½®ä»»ä½• Tracker")

        # é«˜çº§é…ç½®
        print("\nğŸ”§ é«˜çº§é…ç½®:")
        try:
            if hasattr(self.config, 'get_setting'):
                tolerance = self.config.get_setting('file_search_tolerance', 60)
                max_results = self.config.get_setting('max_search_results', 10)
                cache_enabled = self.config.get_setting('enable_cache', True)
                max_concurrent = self.config.get_setting('max_concurrent_operations', 4)
            else:
                # å¦‚æœ get_setting æ–¹æ³•ä¸å­˜åœ¨ï¼Œç›´æ¥ä» settings å­—å…¸è·å–
                tolerance = self.config.settings.get('file_search_tolerance', 60)
                max_results = self.config.settings.get('max_search_results', 10)
                cache_enabled = self.config.settings.get('enable_cache', True)
                max_concurrent = self.config.settings.get('max_concurrent_operations', 4)

            print(f"   ğŸ” æœç´¢å®¹é”™ç‡: {tolerance}%")
            print(f"   ğŸ“Š æœ€å¤§æœç´¢ç»“æœ: {max_results}")
            print(f"   ğŸ’¾ ç¼“å­˜çŠ¶æ€: {'å¯ç”¨' if cache_enabled else 'ç¦ç”¨'}")
            print(f"   âš¡ æœ€å¤§å¹¶å‘æ“ä½œ: {max_concurrent}")

        except Exception as e:
            print(f"   âš ï¸ è·å–è¯¦ç»†é…ç½®ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            print("   åŸºæœ¬é…ç½®ä¿¡æ¯å·²æ˜¾ç¤º")

        # é…ç½®æ–‡ä»¶çŠ¶æ€
        print("\nğŸ“„ é…ç½®æ–‡ä»¶çŠ¶æ€:")
        if hasattr(self.config, 'settings_path'):
            settings_path = self.config.settings_path
            trackers_path = self.config.trackers_path
            print(f"   âš™ï¸ è®¾ç½®æ–‡ä»¶: {settings_path}")
            print(f"      {'âœ… å­˜åœ¨' if os.path.exists(settings_path) else 'âŒ ä¸å­˜åœ¨'}")
            print(f"   ğŸŒ Trackeræ–‡ä»¶: {trackers_path}")
            print(f"      {'âœ… å­˜åœ¨' if os.path.exists(trackers_path) else 'âŒ ä¸å­˜åœ¨'}")
        else:
            print("   ğŸ“ é…ç½®ç›®å½•: ~/.torrent_maker/")

        print("=" * 60)

    def _set_resource_folder(self):
        """è®¾ç½®èµ„æºæ–‡ä»¶å¤¹"""
        print(f"\nğŸ“ å½“å‰èµ„æºæ–‡ä»¶å¤¹: {self.config.get_resource_folder()}")
        new_path = input("è¯·è¾“å…¥æ–°çš„èµ„æºæ–‡ä»¶å¤¹è·¯å¾„ (å›è½¦å–æ¶ˆ): ").strip()
        if new_path:
            if self.config.set_resource_folder(new_path):
                print("âœ… èµ„æºæ–‡ä»¶å¤¹è®¾ç½®æˆåŠŸ")
                # é‡æ–°åˆå§‹åŒ–æ–‡ä»¶åŒ¹é…å™¨
                enable_cache = True
                cache_duration = 3600
                max_workers = 4

                if hasattr(self.config, 'get_setting'):
                    enable_cache = self.config.get_setting('enable_cache', True)
                    cache_duration = self.config.get_setting('cache_duration', 3600)
                    max_workers = self.config.get_setting('max_concurrent_operations', 4)
                elif hasattr(self.config, 'settings'):
                    enable_cache = self.config.settings.get('enable_cache', True)
                    cache_duration = self.config.settings.get('cache_duration', 3600)
                    max_workers = self.config.settings.get('max_concurrent_operations', 4)

                # ä½¿ç”¨æ–°è®¾ç½®çš„è·¯å¾„ç›´æ¥åˆ›å»º FileMatcher
                new_resource_folder = self.config.settings['resource_folder']
                self.matcher = FileMatcher(
                    new_resource_folder,
                    enable_cache=enable_cache,
                    cache_duration=cache_duration,
                    max_workers=max_workers
                )
                print(f"ğŸ”„ æ–‡ä»¶åŒ¹é…å™¨å·²é‡æ–°åˆå§‹åŒ–ï¼Œä½¿ç”¨è·¯å¾„: {new_resource_folder}")
            else:
                print("âŒ è®¾ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨")

    def _set_output_folder(self):
        """è®¾ç½®è¾“å‡ºæ–‡ä»¶å¤¹"""
        print(f"\nğŸ“‚ å½“å‰è¾“å‡ºæ–‡ä»¶å¤¹: {self.config.get_output_folder()}")
        new_path = input("è¯·è¾“å…¥æ–°çš„è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ (å›è½¦å–æ¶ˆ): ").strip()
        if new_path:
            if self.config.set_output_folder(new_path):
                print("âœ… è¾“å‡ºæ–‡ä»¶å¤¹è®¾ç½®æˆåŠŸ")
                # é‡æ–°åˆå§‹åŒ–ç§å­åˆ›å»ºå™¨
                self.creator = TorrentCreator(
                    self.config.get_trackers(),
                    self.config.get_output_folder()
                )
            else:
                print("âŒ è®¾ç½®å¤±è´¥")

    def _manage_trackers(self):
        """ç®¡ç† Tracker"""
        while True:
            print("\nğŸŒ Tracker ç®¡ç†")
            print("=" * 30)
            trackers = self.config.get_trackers()
            if trackers:
                for i, tracker in enumerate(trackers, 1):
                    print(f"  {i:2d}. {tracker}")
            else:
                print("  (æ—  Tracker)")

            print("\næ“ä½œé€‰é¡¹:")
            print("1. â• æ·»åŠ  Tracker")
            print("2. â– åˆ é™¤ Tracker")
            print("0. ğŸ”™ è¿”å›")

            choice = input("\nè¯·é€‰æ‹©æ“ä½œ (0-2): ").strip()

            if choice == '0':
                break
            elif choice == '1':
                tracker_url = input("è¯·è¾“å…¥ Tracker URL: ").strip()
                if tracker_url:
                    if self.config.add_tracker(tracker_url):
                        print("âœ… Tracker æ·»åŠ æˆåŠŸ")
                        # æ›´æ–°ç§å­åˆ›å»ºå™¨çš„ tracker åˆ—è¡¨
                        self.creator = TorrentCreator(
                            self.config.get_trackers(),
                            self.config.get_output_folder()
                        )
                    else:
                        print("âŒ æ·»åŠ å¤±è´¥ï¼Œå¯èƒ½æ˜¯æ— æ•ˆURLæˆ–å·²å­˜åœ¨")
            elif choice == '2':
                if not trackers:
                    print("âŒ æ²¡æœ‰å¯åˆ é™¤çš„ Tracker")
                    continue
                try:
                    idx = int(input("è¯·è¾“å…¥è¦åˆ é™¤çš„ Tracker ç¼–å·: ").strip())
                    if 1 <= idx <= len(trackers):
                        tracker_to_remove = trackers[idx - 1]
                        if self.config.remove_tracker(tracker_to_remove):
                            print("âœ… Tracker åˆ é™¤æˆåŠŸ")
                            # æ›´æ–°ç§å­åˆ›å»ºå™¨çš„ tracker åˆ—è¡¨
                            self.creator = TorrentCreator(
                                self.config.get_trackers(),
                                self.config.get_output_folder()
                            )
                        else:
                            print("âŒ åˆ é™¤å¤±è´¥")
                    else:
                        print("âŒ æ— æ•ˆçš„ç¼–å·")
                except ValueError:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")

    def _reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        try:
            # é‡æ–°åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
            self.config = ConfigManager()

            # é‡æ–°åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
            enable_cache = True
            if hasattr(self.config, 'get_setting'):
                enable_cache = self.config.get_setting('enable_cache', True)
            elif hasattr(self.config, 'settings'):
                enable_cache = self.config.settings.get('enable_cache', True)

            self.matcher = FileMatcher(
                self.config.get_resource_folder(),
                enable_cache=enable_cache
            )

            self.creator = TorrentCreator(
                self.config.get_trackers(),
                self.config.get_output_folder()
            )

            print("âœ… é…ç½®é‡æ–°åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {e}")

    def _export_config(self):
        """å¯¼å‡ºé…ç½®"""
        print("\nğŸ“¤ å¯¼å‡ºé…ç½®")
        print("=" * 30)

        default_path = f"torrent_maker_config_{time.strftime('%Y%m%d_%H%M%S')}.json"
        export_path = input(f"è¯·è¾“å…¥å¯¼å‡ºæ–‡ä»¶è·¯å¾„ (å›è½¦ä½¿ç”¨é»˜è®¤: {default_path}): ").strip()

        if not export_path:
            export_path = default_path

        try:
            if hasattr(self.config, 'export_config'):
                if self.config.export_config(export_path):
                    print(f"âœ… é…ç½®å·²å¯¼å‡ºåˆ°: {export_path}")
                else:
                    print("âŒ å¯¼å‡ºå¤±è´¥")
            else:
                # æ‰‹åŠ¨å¯¼å‡ºé…ç½®
                export_data = {
                    'settings': self.config.settings,
                    'trackers': self.config.get_trackers(),
                    'export_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'version': '1.3.0'
                }

                with open(export_path, 'w', encoding='utf-8') as f:
                    import json
                    json.dump(export_data, f, ensure_ascii=False, indent=4)

                print(f"âœ… é…ç½®å·²å¯¼å‡ºåˆ°: {export_path}")

        except Exception as e:
            print(f"âŒ å¯¼å‡ºé…ç½®å¤±è´¥: {e}")

    def _import_config(self):
        """å¯¼å…¥é…ç½®"""
        print("\nğŸ“¥ å¯¼å…¥é…ç½®")
        print("=" * 30)
        print("âš ï¸ è­¦å‘Šï¼šå¯¼å…¥é…ç½®å°†è¦†ç›–å½“å‰æ‰€æœ‰è®¾ç½®")

        import_path = input("è¯·è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„: ").strip()
        if not import_path:
            print("âŒ è·¯å¾„ä¸èƒ½ä¸ºç©º")
            return

        if not os.path.exists(import_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {import_path}")
            return

        confirm = input("ç¡®è®¤å¯¼å…¥é…ç½®ï¼Ÿè¿™å°†è¦†ç›–å½“å‰è®¾ç½® (y/N): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("âŒ å·²å–æ¶ˆå¯¼å…¥")
            return

        try:
            if hasattr(self.config, 'import_config'):
                if self.config.import_config(import_path):
                    print("âœ… é…ç½®å¯¼å…¥æˆåŠŸ")
                    self._reload_config()  # é‡æ–°åŠ è½½é…ç½®
                else:
                    print("âŒ å¯¼å…¥å¤±è´¥")
            else:
                # æ‰‹åŠ¨å¯¼å…¥é…ç½®
                with open(import_path, 'r', encoding='utf-8') as f:
                    import json
                    import_data = json.load(f)

                if 'settings' in import_data:
                    self.config.settings.update(import_data['settings'])
                    self.config.save_settings()

                if 'trackers' in import_data:
                    self.config.trackers = import_data['trackers']
                    self.config.save_trackers()

                print("âœ… é…ç½®å¯¼å…¥æˆåŠŸ")
                self._reload_config()  # é‡æ–°åŠ è½½é…ç½®

        except Exception as e:
            print(f"âŒ å¯¼å…¥é…ç½®å¤±è´¥: {e}")

    def _reset_config(self):
        """é‡ç½®é…ç½®ä¸ºé»˜è®¤å€¼"""
        print("\nğŸ”„ é‡ç½®é…ç½®")
        print("=" * 30)
        print("âš ï¸ è­¦å‘Šï¼šè¿™å°†é‡ç½®æ‰€æœ‰é…ç½®ä¸ºé»˜è®¤å€¼")
        print("åŒ…æ‹¬ï¼šèµ„æºæ–‡ä»¶å¤¹ã€è¾“å‡ºæ–‡ä»¶å¤¹ã€Trackeråˆ—è¡¨ç­‰")

        confirm = input("ç¡®è®¤é‡ç½®æ‰€æœ‰é…ç½®ä¸ºé»˜è®¤å€¼ï¼Ÿ(y/N): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("âŒ å·²å–æ¶ˆé‡ç½®")
            return

        try:
            if hasattr(self.config, 'reset_to_defaults'):
                if self.config.reset_to_defaults():
                    print("âœ… é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
                    self._reload_config()  # é‡æ–°åŠ è½½é…ç½®
                else:
                    print("âŒ é‡ç½®å¤±è´¥")
            else:
                # æ‰‹åŠ¨é‡ç½®é…ç½®
                self.config.settings = self.config.DEFAULT_SETTINGS.copy()
                self.config.trackers = self.config.DEFAULT_TRACKERS.copy()

                # å±•å¼€ç”¨æˆ·ç›®å½•è·¯å¾„
                self.config.settings['resource_folder'] = os.path.expanduser(
                    self.config.settings['resource_folder']
                )
                self.config.settings['output_folder'] = os.path.expanduser(
                    self.config.settings['output_folder']
                )

                self.config.save_settings()
                self.config.save_trackers()

                print("âœ… é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
                self._reload_config()  # é‡æ–°åŠ è½½é…ç½®

        except Exception as e:
            print(f"âŒ é‡ç½®é…ç½®å¤±è´¥: {e}")

    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        self.display_header()

        while True:
            try:
                self.display_menu()
                choice = input("è¯·é€‰æ‹©æ“ä½œ (0-6): ").strip()

                if choice == '0':
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ Torrent Maker v1.5.0ï¼")
                    break
                elif choice == '1':
                    self.search_and_create()
                elif choice == '2':
                    self.quick_create()
                elif choice == '3':
                    self.batch_create()
                elif choice == '4':
                    self.config_management()
                elif choice == '5':
                    self.show_performance_stats()
                elif choice == '6':
                    self.show_help()
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

                print()

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
                break
            except Exception as e:
                print(f"âŒ ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def show_performance_stats(self):
        """æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)

        # è·å–æ–‡ä»¶åŒ¹é…å™¨çš„æ€§èƒ½ç»Ÿè®¡
        if hasattr(self.matcher, 'performance_monitor'):
            matcher_stats = self.matcher.performance_monitor.get_all_stats()
            if matcher_stats:
                print("ğŸ” æœç´¢æ€§èƒ½:")
                for name, stats in matcher_stats.items():
                    if stats:
                        print(f"  {name}:")
                        print(f"    æ‰§è¡Œæ¬¡æ•°: {stats['count']}")
                        print(f"    å¹³å‡è€—æ—¶: {stats['average']:.3f}s")
                        print(f"    æœ€å¤§è€—æ—¶: {stats['max']:.3f}s")
                        print(f"    æ€»è€—æ—¶: {stats['total']:.3f}s")
                print()

        # è·å–ç§å­åˆ›å»ºå™¨çš„æ€§èƒ½ç»Ÿè®¡
        if hasattr(self.creator, 'performance_monitor'):
            creator_stats = self.creator.performance_monitor.get_all_stats()
            if creator_stats:
                print("ğŸ› ï¸ ç§å­åˆ›å»ºæ€§èƒ½:")
                for name, stats in creator_stats.items():
                    if stats:
                        print(f"  {name}:")
                        print(f"    æ‰§è¡Œæ¬¡æ•°: {stats['count']}")
                        print(f"    å¹³å‡è€—æ—¶: {stats['average']:.3f}s")
                        print(f"    æœ€å¤§è€—æ—¶: {stats['max']:.3f}s")
                        print(f"    æ€»è€—æ—¶: {stats['total']:.3f}s")
                print()

        # è·å–ç¼“å­˜ç»Ÿè®¡
        if hasattr(self.matcher, 'cache') and self.matcher.cache:
            cache_stats = self.matcher.cache.get_stats()
            if cache_stats:
                print("ğŸ’¾ ç¼“å­˜ç»Ÿè®¡:")
                print(f"  æ€»ç¼“å­˜é¡¹: {cache_stats['total_items']}")
                print(f"  æœ‰æ•ˆç¼“å­˜é¡¹: {cache_stats['valid_items']}")
                print(f"  è¿‡æœŸç¼“å­˜é¡¹: {cache_stats['expired_items']}")
                print()

        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        print("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        suggestions = self._generate_performance_suggestions()
        if suggestions:
            for i, suggestion in enumerate(suggestions, 1):
                print(f"  {i}. {suggestion}")
        else:
            print("  å½“å‰æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")

        print("=" * 60)

    def _generate_performance_suggestions(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        suggestions = []

        # æ£€æŸ¥æœç´¢æ€§èƒ½
        if hasattr(self.matcher, 'performance_monitor'):
            search_stats = self.matcher.performance_monitor.get_stats('fuzzy_search')
            if search_stats and search_stats.get('average', 0) > 2.0:
                suggestions.append("æœç´¢è€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®å¢åŠ ç¼“å­˜æ—¶é—´æˆ–å‡å°‘æœç´¢æ·±åº¦")

        # æ£€æŸ¥ç§å­åˆ›å»ºæ€§èƒ½
        if hasattr(self.creator, 'performance_monitor'):
            creation_stats = self.creator.performance_monitor.get_stats('total_torrent_creation')
            if creation_stats and creation_stats.get('average', 0) > 30.0:
                suggestions.append("ç§å­åˆ›å»ºè€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®æ£€æŸ¥ç£ç›˜æ€§èƒ½æˆ–å‡å°‘æ–‡ä»¶æ•°é‡")

        # æ£€æŸ¥ç¼“å­˜ä½¿ç”¨æƒ…å†µ
        if hasattr(self.matcher, 'cache') and self.matcher.cache:
            cache_stats = self.matcher.cache.get_stats()
            if cache_stats and cache_stats.get('valid_items', 0) == 0:
                suggestions.append("ç¼“å­˜æœªè¢«æœ‰æ•ˆåˆ©ç”¨ï¼Œå»ºè®®æ£€æŸ¥ç¼“å­˜é…ç½®")

        return suggestions

    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\nâ“ å¸®åŠ©ä¿¡æ¯")
        print("=" * 50)
        print("ğŸ” æœç´¢åŠŸèƒ½:")
        print("  - æ”¯æŒæ¨¡ç³Šæœç´¢ï¼Œå®¹é”™ç‡é«˜")
        print("  - è‡ªåŠ¨è¯†åˆ«å‰§é›†ä¿¡æ¯")
        print("  - æ™ºèƒ½ç¼“å­˜ï¼Œé‡å¤æœç´¢æ›´å¿«")
        print()
        print("âš¡ å¿«é€Ÿåˆ¶ç§:")
        print("  - ç›´æ¥è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„")
        print("  - æ”¯æŒæ‰¹é‡è·¯å¾„ (ç”¨åˆ†å·åˆ†éš”)")
        print()
        print("ğŸ¯ æ€§èƒ½ä¼˜åŒ–:")
        print("  - å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†")
        print("  - æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ")
        print("  - å†…å­˜ä½¿ç”¨ä¼˜åŒ–")
        print("  - å®æ—¶æ€§èƒ½ç›‘æ§")
        print("=" * 50)


def main():
    """ä¸»å‡½æ•°"""
    try:
        app = TorrentMakerApp()
        app.run()
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
