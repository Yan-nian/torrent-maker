#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Torrent Maker v1.5.0 ç¬¬äºŒé˜¶æ®µä¼˜åŒ–éªŒè¯æµ‹è¯•
ç®€åŒ–ç‰ˆæœ¬ - éªŒè¯æ ¸å¿ƒä¼˜åŒ–åŠŸèƒ½

ä½œè€…ï¼šTorrent Maker Team
ç‰ˆæœ¬ï¼š1.5.0 Stage 2 Verification
"""

import os
import sys
import time
import tempfile
import shutil
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("ğŸ” æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        from torrent_maker import (
            FileMatcher, DirectorySizeCache, SmartIndexCache, 
            MemoryManager, AsyncIOProcessor, FastSimilarityCalculator
        )
        print("  âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"  âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_smart_search_optimization():
    """æµ‹è¯•æ™ºèƒ½æœç´¢ä¼˜åŒ–"""
    print("\nğŸ¯ æµ‹è¯•æ™ºèƒ½æœç´¢ä¼˜åŒ–...")
    
    try:
        from torrent_maker import FileMatcher, SmartIndexCache
        
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
        test_dir = tempfile.mkdtemp()
        test_folders = [
            "The Matrix 1999",
            "Matrix Reloaded 2003", 
            "Matrix Revolutions 2003",
            "Breaking Bad S01",
            "Breaking Bad S02",
            "Avengers Endgame 2019",
            "Spider Man No Way Home 2021"
        ]
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶å¤¹
        for folder in test_folders:
            os.makedirs(os.path.join(test_dir, folder), exist_ok=True)
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        matcher = FileMatcher(test_dir, enable_cache=True)
        
        # æµ‹è¯•æœç´¢æŸ¥è¯¢
        search_queries = ["Matrix", "Breaking", "Avengers"]
        results = {}
        
        for query in search_queries:
            print(f"  æœç´¢: '{query}'")
            start_time = time.time()
            matches = matcher.fuzzy_search(query, max_results=5)
            search_time = time.time() - start_time
            
            results[query] = {
                'matches': len(matches),
                'time': search_time,
                'results': [match[0] for match in matches[:3]]
            }
            
            print(f"    æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…ï¼Œè€—æ—¶ {search_time:.4f}s")
        
        # æ¸…ç†
        shutil.rmtree(test_dir)
        
        print("  âœ… æ™ºèƒ½æœç´¢ä¼˜åŒ–æµ‹è¯•é€šè¿‡")
        return results
        
    except Exception as e:
        print(f"  âŒ æ™ºèƒ½æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        return {}

def test_memory_management():
    """æµ‹è¯•å†…å­˜ç®¡ç†"""
    print("\nğŸ’¾ æµ‹è¯•å†…å­˜ç®¡ç†...")
    
    try:
        from torrent_maker import MemoryManager
        
        # åˆ›å»ºå†…å­˜ç®¡ç†å™¨
        memory_manager = MemoryManager(max_memory_mb=128)
        
        # è·å–åˆå§‹å†…å­˜çŠ¶æ€
        initial_memory = memory_manager.get_memory_usage()
        print(f"  åˆå§‹å†…å­˜: {initial_memory.get('rss_mb', 0):.1f}MB")
        
        # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
        large_data = []
        for i in range(50):
            large_data.append([f"test_data_{j}" for j in range(500)])
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        current_memory = memory_manager.get_memory_usage()
        print(f"  ä½¿ç”¨åå†…å­˜: {current_memory.get('rss_mb', 0):.1f}MB")
        
        # æµ‹è¯•å†…å­˜æ¸…ç†
        cleanup_result = memory_manager.cleanup_memory()
        print(f"  æ¸…ç†ç»“æœ: GCå›æ”¶ {cleanup_result.get('gc_collected', 0)} ä¸ªå¯¹è±¡")
        
        # æ¸…ç†æ•°æ®
        del large_data
        
        final_memory = memory_manager.get_memory_usage()
        print(f"  æœ€ç»ˆå†…å­˜: {final_memory.get('rss_mb', 0):.1f}MB")
        
        print("  âœ… å†…å­˜ç®¡ç†æµ‹è¯•é€šè¿‡")
        return {
            'initial_mb': initial_memory.get('rss_mb', 0),
            'peak_mb': current_memory.get('rss_mb', 0),
            'final_mb': final_memory.get('rss_mb', 0),
            'cleanup_items': cleanup_result.get('gc_collected', 0)
        }
        
    except Exception as e:
        print(f"  âŒ å†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return {}

def test_directory_cache():
    """æµ‹è¯•ç›®å½•ç¼“å­˜ä¼˜åŒ–"""
    print("\nğŸ“ æµ‹è¯•ç›®å½•ç¼“å­˜ä¼˜åŒ–...")
    
    try:
        from torrent_maker import DirectorySizeCache
        
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºä¸€äº›æµ‹è¯•æ–‡ä»¶
        for i in range(10):
            file_path = os.path.join(test_dir, f"test_file_{i}.txt")
            with open(file_path, 'w') as f:
                f.write("test content " * 100)  # çº¦1KBæ–‡ä»¶
        
        # åˆ›å»ºç¼“å­˜å®ä¾‹
        cache = DirectorySizeCache(cache_duration=300, max_cache_size=100)
        
        # ç¬¬ä¸€æ¬¡è®¡ç®—ï¼ˆæ— ç¼“å­˜ï¼‰
        start_time = time.time()
        size1 = cache.get_directory_size(Path(test_dir))
        time1 = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆæœ‰ç¼“å­˜ï¼‰
        start_time = time.time()
        size2 = cache.get_directory_size(Path(test_dir))
        time2 = time.time() - start_time
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = cache.get_cache_stats()
        
        print(f"  ç¬¬ä¸€æ¬¡è®¡ç®—: {size1} bytes, è€—æ—¶ {time1:.4f}s")
        print(f"  ç¬¬äºŒæ¬¡è®¡ç®—: {size2} bytes, è€—æ—¶ {time2:.4f}s")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 0):.2%}")
        print(f"  åŠ é€Ÿæ¯”: {time1/time2 if time2 > 0 else 'N/A':.1f}x")
        
        # æ¸…ç†
        shutil.rmtree(test_dir)
        
        print("  âœ… ç›®å½•ç¼“å­˜ä¼˜åŒ–æµ‹è¯•é€šè¿‡")
        return {
            'first_time': time1,
            'second_time': time2,
            'speedup': time1/time2 if time2 > 0 else 0,
            'hit_rate': cache_stats.get('hit_rate', 0)
        }
        
    except Exception as e:
        print(f"  âŒ ç›®å½•ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        return {}

def test_async_io():
    """æµ‹è¯•å¼‚æ­¥I/Oå¤„ç†"""
    print("\nâš¡ æµ‹è¯•å¼‚æ­¥I/Oå¤„ç†...")
    
    try:
        from torrent_maker import AsyncIOProcessor
        
        # åˆ›å»ºå¼‚æ­¥å¤„ç†å™¨
        async_processor = AsyncIOProcessor(max_concurrent=4)
        
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºå­ç›®å½•ç»“æ„
        for i in range(5):
            subdir = os.path.join(test_dir, f"subdir_{i}")
            os.makedirs(subdir, exist_ok=True)
            
            # åœ¨æ¯ä¸ªå­ç›®å½•åˆ›å»ºæ–‡ä»¶
            for j in range(3):
                file_path = os.path.join(subdir, f"file_{j}.txt")
                with open(file_path, 'w') as f:
                    f.write("async test content " * 50)
        
        # æµ‹è¯•å¼‚æ­¥ç›®å½•æ‰«æ
        start_time = time.time()
        folders = async_processor.async_directory_scan(Path(test_dir), max_depth=2)
        scan_time = time.time() - start_time
        
        print(f"  å¼‚æ­¥æ‰«æ: æ‰¾åˆ° {len(folders)} ä¸ªæ–‡ä»¶å¤¹ï¼Œè€—æ—¶ {scan_time:.4f}s")
        
        # æ¸…ç†èµ„æº
        async_processor.cleanup()
        shutil.rmtree(test_dir)
        
        print("  âœ… å¼‚æ­¥I/Oå¤„ç†æµ‹è¯•é€šè¿‡")
        return {
            'folders_found': len(folders),
            'scan_time': scan_time
        }
        
    except Exception as e:
        print(f"  âŒ å¼‚æ­¥I/Oæµ‹è¯•å¤±è´¥: {e}")
        return {}

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Torrent Maker v1.5.0 ç¬¬äºŒé˜¶æ®µä¼˜åŒ–éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    results = {}
    
    # 1. æµ‹è¯•æ¨¡å—å¯¼å…¥
    if not test_imports():
        print("\nâŒ æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return
    
    # 2. æµ‹è¯•å„é¡¹ä¼˜åŒ–åŠŸèƒ½
    results['search_optimization'] = test_smart_search_optimization()
    results['memory_management'] = test_memory_management()
    results['directory_cache'] = test_directory_cache()
    results['async_io'] = test_async_io()
    
    # 3. è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š ç¬¬äºŒé˜¶æ®µä¼˜åŒ–éªŒè¯ç»“æœæ€»ç»“:")
    print("=" * 60)
    
    # æœç´¢ä¼˜åŒ–ç»“æœ
    if results['search_optimization']:
        print("ğŸ¯ æ™ºèƒ½æœç´¢ä¼˜åŒ–: âœ… é€šè¿‡")
        search_data = results['search_optimization']
        avg_time = sum(data['time'] for data in search_data.values()) / len(search_data)
        print(f"   å¹³å‡æœç´¢æ—¶é—´: {avg_time:.4f}s")
    
    # å†…å­˜ç®¡ç†ç»“æœ
    if results['memory_management']:
        print("ğŸ’¾ å†…å­˜ç®¡ç†ä¼˜åŒ–: âœ… é€šè¿‡")
        mem_data = results['memory_management']
        print(f"   å†…å­˜ä½¿ç”¨: {mem_data['initial_mb']:.1f}MB â†’ {mem_data['final_mb']:.1f}MB")
    
    # ç›®å½•ç¼“å­˜ç»“æœ
    if results['directory_cache']:
        print("ğŸ“ ç›®å½•ç¼“å­˜ä¼˜åŒ–: âœ… é€šè¿‡")
        cache_data = results['directory_cache']
        print(f"   ç¼“å­˜åŠ é€Ÿæ¯”: {cache_data['speedup']:.1f}x")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {cache_data['hit_rate']:.2%}")
    
    # å¼‚æ­¥I/Oç»“æœ
    if results['async_io']:
        print("âš¡ å¼‚æ­¥I/Oå¤„ç†: âœ… é€šè¿‡")
        async_data = results['async_io']
        print(f"   æ‰«ææ€§èƒ½: {async_data['folders_found']} æ–‡ä»¶å¤¹ï¼Œ{async_data['scan_time']:.4f}s")
    
    print("\nğŸ‰ ç¬¬äºŒé˜¶æ®µä¼˜åŒ–éªŒè¯å®Œæˆï¼")
    print("ğŸš€ v1.5.0 ç¬¬äºŒé˜¶æ®µä¼˜åŒ–æ•ˆæœ:")
    print("   ğŸ¯ æ™ºèƒ½æœç´¢é¢„ç­›é€‰æœºåˆ¶")
    print("   ğŸ’¾ æ·±åº¦å†…å­˜ç®¡ç†å’Œç›‘æ§")
    print("   ğŸ“ LRUç¼“å­˜ç³»ç»Ÿä¼˜åŒ–")
    print("   âš¡ å¼‚æ­¥I/Oå¹¶å‘å¤„ç†")

if __name__ == "__main__":
    main()
